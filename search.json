[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " PO11Q: Introduction to Quantitative Political Analysis I",
    "section": "",
    "text": "Welcome!\nA vast amount of political research is quantitative, and even if you decide never to conduct quantitative analysis yourself, you will find an introductory level of knowledge in quantitative methods useful to critically engage with your discipline as a whole. Skills in data analysis are also crucial for finding employment in graduate-level jobs.\nThis module will deliver an introduction to quantitative political analysis. It is based on Linke’s typology of Quantitative Methods, and explores each of its tasks (conceptualisation and measurement, numerical data, data analysis, and interpretation) at an introductory level. The module uses the software R.\nThis is the online companion to the seminars on PO11Q. It hosts all of the material needed for the seminars and replaces the physical worksheets which we work through in some of the seminars. This is environmentally friendlier, and once we get to working with R, it also allows you to copy/paste code directly from the code chunks in this online companion.\nI hope you find this useful!\n\n \n\n\n\nCompanion Features\nYou will find embedded in the text six different types of boxes which serve different purposes:\n\nThis box appears whenever I want you to stop at a particular point in the worksheet and to flag up to me that you are done.\n\n\nThis appears when you need to be careful with your coding in R to avoid problems.\n\n\nSome explanations that will hopefully make your work with this webpage or learning the material itself easier.\n\n\nWe will start working with R in week 5, and I have recorded some videos to ease you into working with the program.\n\n\nA brief question which tests your understanding of the previous material.\n\n\nA definition that i simportant for the current discussion.\n\n\n\nAccessibility\nThe companion uses the font “Lexend”. Lexend fonts are intended to reduce visual stress and so improve reading performance. Initially they were designed with dyslexia and struggling readers in mind, but Bonnie Shaver-Troup, creator of the Lexend project, soon found out that these fonts are also great for everyone else.\nCode is displayed in Recursive Mono. The font’s characters share the same width for clear legibility and perfect alignment. This is particularly helpful for use in programming and data-heavy design tasks, but also allows for creative possibilities in display typography.\nEquations and mathematical expressions are set in Fira Math. Fira Math is easy to read with open shapes, clear spacing, and good contrast. This keeps subscripts, superscripts, and fractions legible - even at small sizes. It stays readable on screens and in print, reducing mix-ups between look-alike characters like 1, I, and l.\nIf the three-column layout feels too busy, click the small text icon in the companion’s top-right to switch to Reader Mode. This streamlines the page and minimises the table of contents into a compact bar at the top.\nThe companion also uses a dark mode theme. For many users, including some neurodivergent individuals, dark mode can reduce eye strain and enhance focus by minimising visual overstimulation.\nWhilst I recognise that mobile phones play an essential part in all our daily lives, please note that this companion is optimised for display on tablets, laptops, and desktop computers.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome</span>"
    ]
  },
  {
    "objectID": "01-PO11Q.html",
    "href": "01-PO11Q.html",
    "title": "Week 1",
    "section": "",
    "text": "No Seminar\nAs this is a first-year module, there is no seminar in Week 1. But you can find the full glossary for this week below.\n\n\nGlossary\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDescription\n\n\n\n\nanalysis\nA detailed evaluation of data to discover their structure and relevant information to answer a research question\n\n\ndata\nDerives from the Latin which means . For our purposes it is a collection of numbers (or quantities) for the purpose of analysis\n\n\ninterpretation\nThe explanation of results to answer the research question\n\n\nliterature review\nAn analytical summary of the literature relating to a particular topic with the objective of identifying a gap and thus motivating a research question\n\n\nmethod\nA tool for systematic investigation\n\n\nQM\nThe process of testing theoretical propositions through the analysis of numerical data in order to provide an answer to a research question\n\n\nresearch question\nA specific enquiry relating to a particular topic or subject. It forms the starting point of the research cycle\n\n\nSocial Sciences\nAre concerned with the study of society and seek to scientifically describe and explain the behaviour of actors\n\n\ntheory\nA formal set of ideas that is intended to explain why something happens or exists (Oxford Learner’s Dictionaries, n.d.)\n\n\n\n\n\n\nTable 1: Glossary Week 1\n\n\n\n\n\n \n\n\n\n\nOxford Learner’s Dictionaries. (n.d.). https://www.oxfordlearnersdictionaries.com/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html",
    "href": "02-PO11Q.html",
    "title": "Week 2",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#the-global-view",
    "href": "02-PO11Q.html#the-global-view",
    "title": "Week 2",
    "section": "The Global View",
    "text": "The Global View\n\nThe content of the module is organised into weekly folders",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#the-weekly-view",
    "href": "02-PO11Q.html#the-weekly-view",
    "title": "Week 2",
    "section": "The Weekly View",
    "text": "The Weekly View\n\nIn each folder, you will find the labels\n\n“Learning Outcomes”\n“Before the Lecture”\n“Lecture Materials”\n“Before the Seminar”\n“Seminar”\n“Test Your Knowledge”\n“Useful Stuff” (sometimes)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#learning-outcomes",
    "href": "02-PO11Q.html#learning-outcomes",
    "title": "Week 2",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nFor each week you will find the learning outcomes at the top\nThese relate to the lecture, the seminar, and the reading",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#before-the-lecture",
    "href": "02-PO11Q.html#before-the-lecture",
    "title": "Week 2",
    "section": "Before the Lecture",
    "text": "Before the Lecture\n\nHere you will find the preparation you are required to complete BEFORE coming to the lecture.\nHere you will also find a link to the reading list detailing the readings you are expected to complete BEFORE the lecture.\nThe slides for each lecture will be available here on the previous Friday (latest).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#lecture-materials",
    "href": "02-PO11Q.html#lecture-materials",
    "title": "Week 2",
    "section": "Lecture Materials",
    "text": "Lecture Materials\n\nThe slides for each lecture will be available here on the previous Friday (latest).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#before-the-seminar",
    "href": "02-PO11Q.html#before-the-seminar",
    "title": "Week 2",
    "section": "Before the Seminar",
    "text": "Before the Seminar\n\nHere is a link to Talis and the readings you are expected to complete before the seminar\nA link to this companion and the list of tasks you are expected to carry out as homework from the previous week",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#seminar",
    "href": "02-PO11Q.html#seminar",
    "title": "Week 2",
    "section": "Seminar",
    "text": "Seminar\n\nHere you will find the link to the relevant week in this companion",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#test-your-knowledge",
    "href": "02-PO11Q.html#test-your-knowledge",
    "title": "Week 2",
    "section": "Test Your Knowledge",
    "text": "Test Your Knowledge\n\nThese quizzes are the formative assessment of the module\nAs such, it gives you an overview of your progress\nIt also gives Flo an overview of gaps and problematic areas\nThe odd question might find its way into the exam\nIt is non-assessed, but very important to complete",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#what-can-i-find-under-the-label-useful-stuff",
    "href": "02-PO11Q.html#what-can-i-find-under-the-label-useful-stuff",
    "title": "Week 2",
    "section": "What can I find under the label “Useful Stuff”?",
    "text": "What can I find under the label “Useful Stuff”?\n\nDo you know this one drawer in your house that contains all sorts of stuff?\nThis is the “Useful Stuff” label\nThe things in there are relevant for the topics we cover, but is complementary",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#fora",
    "href": "02-PO11Q.html#fora",
    "title": "Week 2",
    "section": "Fora",
    "text": "Fora\n\nThere are a number of fora at the top of the Moodle page\nPlease use them as intended",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#where-can-i-find-information-on-the-assessment",
    "href": "02-PO11Q.html#where-can-i-find-information-on-the-assessment",
    "title": "Week 2",
    "section": "Where can I find information on the assessment?",
    "text": "Where can I find information on the assessment?\n\nThe module has two assessments\nInformation on these can be found in the “Assessment” folder on the Moodle page",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#how-do-i-find-a-reading",
    "href": "02-PO11Q.html#how-do-i-find-a-reading",
    "title": "Week 2",
    "section": "How do I find a reading?",
    "text": "How do I find a reading?\n\nAvailable through Moodle",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#how-do-i-find-a-reading-contd.",
    "href": "02-PO11Q.html#how-do-i-find-a-reading-contd.",
    "title": "Week 2",
    "section": "How do I find a reading? (contd.)",
    "text": "How do I find a reading? (contd.)\n\nScroll to the appropriate item\nIf you see an icon labelled on the right, click on it\nYou will be asked to log in with your university credentials\nYou can then click yourself through to the actual document",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#what-if-the-reading-is-not-digitally-available",
    "href": "02-PO11Q.html#what-if-the-reading-is-not-digitally-available",
    "title": "Week 2",
    "section": "What if the reading is not digitally available?",
    "text": "What if the reading is not digitally available?\n\nThen, I’m afraid, you will have to go to the library in person",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#essential-versus-recommended-reading",
    "href": "02-PO11Q.html#essential-versus-recommended-reading",
    "title": "Week 2",
    "section": "Essential versus Recommended Reading",
    "text": "Essential versus Recommended Reading\n\nEssential: This reading is compulsory, and the material covered in it is relevant for the exam.\nRecommended: If you want a different take on the same topic, or you want some more information, then read these materials.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#does-it-make-sense-to-buy-a-book",
    "href": "02-PO11Q.html#does-it-make-sense-to-buy-a-book",
    "title": "Week 2",
    "section": "Does it make sense to buy a book?",
    "text": "Does it make sense to buy a book?\n\nYes, there are two core texts for this module:\n\nFogarty (2023)\nAgresti (2018)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#group-work-1",
    "href": "02-PO11Q.html#group-work-1",
    "title": "Week 2",
    "section": "Group Work 1",
    "text": "Group Work 1\n\nHow would you measure the size of the European Union?\nCome together in small groups (3-4)\nTry to identify ways in which you would measure the size of the European Union",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO11Q.html#group-work-2",
    "href": "02-PO11Q.html#group-work-2",
    "title": "Week 2",
    "section": "Group Work 2",
    "text": "Group Work 2\n\nConsider the measurements you developed for the size of the EU\n\nIdentify which of the items are attributes and which are measurements?\nDraw the conceptualisation tree and sort your items into attributes (branches) and measurements (leaves)\nComplete the tree so that all branches have leaves and and all leaves are attached to a branch.\n\nPrepare a short presentation of your results",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "03-PO11Q.html",
    "href": "03-PO11Q.html",
    "title": "Week 3",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO11Q.html#references-how",
    "href": "03-PO11Q.html#references-how",
    "title": "Week 3",
    "section": "References: How?",
    "text": "References: How?\n\nThe expectation is that you provide at least ten unique references, and include material beyond the reading list.\nWhen should you include references?\n\nWhen you refer to an idea or argument that is not originally yours\nWhen you refer to statistics, data, or other claims (‘statements of fact’)\nWhen you are quoting someone\n\nThe PAIS Undergraduate Handbook and the academic skills pages should always be your first port of call for this module.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO11Q.html#references-harvard",
    "href": "03-PO11Q.html#references-harvard",
    "title": "Week 3",
    "section": "References: Harvard",
    "text": "References: Harvard\nYou can only use Harvard as referencing system:\n\nAs Dahl (1989) argues, …\nThe notion of who the people are in a democracy is not clearly defined (Dahl, 1989, p. 2)\n…\n\nDahl, R. A. (1989). Democracy and its Critics. New Haven, CT: Yale University Press.\n\nPlease remember that only in-text citations are permissible for this assessment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO11Q.html#quotations",
    "href": "03-PO11Q.html#quotations",
    "title": "Week 3",
    "section": "Quotations",
    "text": "Quotations\nQuotation marks indicate the beginning and end of each quotation. Use double quotation marks for this purpose. But if the quotation contains a further quotation within itself, then use single quotation marks to indicate the inner quotation.\nExample:\n\nAccording to Parker (2000, p. 284), “An editorial from the Ontario Women’s Association Newsletter states that ‘There are no women’s issues, they should be called family issues’”.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO11Q.html#long-quotations",
    "href": "03-PO11Q.html#long-quotations",
    "title": "Week 3",
    "section": "Long Quotations",
    "text": "Long Quotations\nQuotations longer than 40 words are called block quotations and deserve special treatment. The quoted passage does not need quotation marks but it should be indented, i.e. the line length should be reduced by about 1cm at both ends.\n\nBibliography vs. List of References\n\nReferences: Only contains the works cited in the text.\nBibliography: Contains the works cited in the text as well asthe sources you have used for background reading without including them in your output\n\nThe PAIS UG Handbook refers to it as a bibliography, but has a list of references in mind. You know better now. Please name it “References”.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO11Q.html#list-of-references",
    "href": "03-PO11Q.html#list-of-references",
    "title": "Week 3",
    "section": "List of References",
    "text": "List of References\n\nDoes not count towards the word limit\nSorted alphabetically by surname of author\nNo bullet points\nNo numbering\nNo creativity",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO11Q.html#footnotes",
    "href": "03-PO11Q.html#footnotes",
    "title": "Week 3",
    "section": "",
    "text": "Some of the content of this worksheet is taken from Linke (forthcoming).↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "04-PO11Q.html",
    "href": "04-PO11Q.html",
    "title": "Week 4",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO11Q.html#videos",
    "href": "04-PO11Q.html#videos",
    "title": "Week 4",
    "section": "Videos",
    "text": "Videos\nBy popular request from students in the past, here are two short videos in which I explain how to calculate the mean and the standard deviation. I have gone through this in the lecture, but if you want a refresher before doing the calculations below, feel free to watch these (with earphones if you are in the seminar).\n\nCalculating a Mean\n\n\n\n\nStandard Deviation",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO11Q.html#data-set",
    "href": "04-PO11Q.html#data-set",
    "title": "Week 4",
    "section": "Data Set",
    "text": "Data Set\n \n\n\n\n\n\n\n\n\n\n\ni\nincome\nage\n\n\n\n\n1\n450\n21\n\n\n2\n550\n23\n\n\n3\n300\n27\n\n\n4\n650\n30\n\n\n5\n100\n20\n\n\n6\n900\n18\n\n\n7\n200\n20\n\n\n8\n250\n22\n\n\n9\n300\n21\n\n\n10\n600\n21\n\n\n\n\n\n\nTable 1: Week 4 Data Set",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO11Q.html#exercises",
    "href": "04-PO11Q.html#exercises",
    "title": "Week 4",
    "section": "Exercises",
    "text": "Exercises\nCalculate the following descriptive statistics for each variable in Table 1:\n\nMean\nMode\nMedian\nRange\nStandard Deviation\nVariance\nthe \\(70^{\\text{th}}\\) percentile",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO11Q.html#solutions",
    "href": "04-PO11Q.html#solutions",
    "title": "Week 4",
    "section": "Solutions",
    "text": "Solutions\nYou can find the Solutions here or in the Downloads Section.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO11Q.html#practice-calculations",
    "href": "04-PO11Q.html#practice-calculations",
    "title": "Week 4",
    "section": "Practice Calculations",
    "text": "Practice Calculations\nI recommend practicing these calculations (exam relevant). To this end, I have written a small application that will help you check the calculations for any data set up to five observations. Just click the button “Random 5” in the app and you will get a new data set to work with.\n\n  Open Application",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO11Q.html#footnotes",
    "href": "04-PO11Q.html#footnotes",
    "title": "Week 4",
    "section": "",
    "text": "Some of the content of this worksheet is taken from Linke (forthcoming).↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html",
    "href": "05-PO11Q.html",
    "title": "Week 5",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#r-rstudio-installation",
    "href": "05-PO11Q.html#r-rstudio-installation",
    "title": "Week 5",
    "section": "R & RStudio – Installation",
    "text": "R & RStudio – Installation\nToday we start working with R and the first step is to install the program. Please follow these instructions:\n\nDownload R. Follow on-screen instructions and install the program.\nDownload RStudio Desktop. Install the program.\nNow open RStudio - you do not need to open R itself, as we will be operating it through RStudio.\n\n\nWhilst you need to install both R and RStudio, we will never be working with R directly. Instead, we will be operating it through RStudio.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#r---getting-started",
    "href": "05-PO11Q.html#r---getting-started",
    "title": "Week 5",
    "section": "R - Getting Started",
    "text": "R - Getting Started\nIn this worksheet and also in all other presentations and documents I use on this module, I am using two different fonts:\n\nFont for plain text\nA typewriter font for R functions, values, etc.\n\nI am also regularly including “screenshots” of operations in R with their output. Whenever you see these, please replicate them on your own computer. To start, let’s have a look at RStudio itself. When you open the programme, you are presented with the following screen:\n\n\n\n\n\n\nFigure 1: RStudio\n\n\n\nIt has – for now – three components to it. On the left hand-side you see the so-called Console into which you can enter the commands, and in which also most of the results will be displayed. On the right hands side, you see the Workspace which consists of an upper and a lower window. The upper window has three tabs in it. The tab Environment will provide you with a list of all the data sets you have loaded into R, and also of the objects and values you create (more on that later). Under the History tab, you find a history (I know, who would have thought it) of all the commands you have used. This can be very useful to retrace your steps. In the Connections tab you can connect to online sources. We will not use this tab.\nIn the lower window, you have five tabs. Under Files you find the file structure of your computer. Once you have set a working directory (more on that in a moment), you can also view the files in your working directory here which gives you a good overview of the files you need to refer to for a particular project. The Plots tab will display the graphs we will be producing. Packages form the heart and soul of R and they make the program as powerful as it is (again, more on that later). RStudio also has a Help function, which is rarely very illuminating. I usually search for stuff online on “stackexchange”, as there is a large community of R users out there who share their knowledge and solutions to problems. We won’t use the last tab Viewer.\n\nIntroduction to R Studio\nIf you can’t get enough of my delightful German accent, then I have some videos for you in which I go through the respective components of the worksheet on screen. Here is the first:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#rscript",
    "href": "05-PO11Q.html#rscript",
    "title": "Week 5",
    "section": "RScript",
    "text": "RScript\nIf you read the previous section carefully, you will have noticed that I wrote that you can enter the commands” in the Console. You can, but you shouldn’t. What you should be using instead is an RScript. An RScript is a list of commands you use for a project (an essay, your dissertation, an article) to calculate quantities of interest, such as descriptive statistics in the form of mean, median and mode, and produce graphs.\nOne of the foundations of scientific research is “reproducibility”“, or”replicability”. This means that “sufficient information exists with which to understand, evaluate, and build upon a prior work if a third party could replicate the results without any additional information from the author.” King (1995, p. 444, emphasis removed) This principle applies in academia more generally, because only if you understand what a person has done before you, you can pick their work up whether they left it, and push the boundaries of knowledge further. But a bit closer to home, it is also relevant for conducting quantitative research in assessments. We require you to submit an RScript (or a “do file” if you use Stata) now together with your actual essay. This is not only to check what you have done; data preparation is often the most time-consuming part (as you will soon discover), and this is a way to gain recognition for this work. So it is actually to your advantage, and not a mere plagiarism check.\nThe creation of an RScript will allow you to open the raw data, and by running the script, to bring it to exactly where you left off. This saves you saving data sets which can take up a lot of work. If you back the script up properly, you also have an insurance against losing all your work a day before the assessment is due.\nTo create an RScript, click File \\(\\rightarrow\\) New File \\(\\rightarrow\\) RScript. A fourth window opens, and your screen will now look something like this:\n\n\n\n\n\n\nFigure 2: The RScript Window\n\n\n\nYou can now write your commands in the RScript, where a new line (for now) means a new command. If you want to execute a command, put the cursor on the line the command is on and press “command” / “enter” simultaneoulsy on a Mac and “Ctrl” / “Enter” on Windows.\nIf you precede a line with #, you can write annotations to yourself, for example explaining what you do with a particular command. More on this in the next sub-section.\nFigure 3 shows the start of the RScript for this worksheet. I prefer a dark background, it’s easier on the eyes, especially when you work with R for long periods. You can change the settings in: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Appearance \\(\\rightarrow\\) Twilight.\n\n\n\n\n\n\nFigure 3: Example of an RScript\n\n\n\n\nMore Themes\nIf you copy and paste the following code chunks into your “Console” and run one at a time, you will have even more themes2 to choose from:\n\ninstall.packages(\n  \"rsthemes\",\n  repos = c(gadenbuie = 'https://gadenbuie.r-universe.dev', getOption(\"repos\"))\n)\n\nrsthemes::install_rsthemes()\n\nYou can also download Flo’s Dark Theme3 and then “add” it at the bottom of the “Appearance” menu.\n\n\nAppearance\n\n\n\n\nRScript Structure\nWell, I am German, and I like things neat and tidy, so I feel almost compelled to discuss how to properly organise an RScript. But apart from genetical dispositions, a well-organised RScript is also very much in the spirit of reproducibility. It simply makes sense to structure an RScript in such a way that another researcher is able to easily read and understand it.\nFirst of all, which commands to include? If you introduce me to your current girlfriend or boyfriend, I have no interest in learning about all your past relationships; they have not worked out. In a similar fashion, nobody wants to read through lines of code that are irrelvant. So you will only include in the RScript those commands which produce the output you actually include in the essay or article.\nI stated above that if you precede a line with #, you can write annotations to yourself. This is also a useful way to structure an RScript, for example into exercise numbers, sections of an essay /article, or different stages of data preparation (which we will be doing in due course).\n\nRScript Structure",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#first-steps-in-r",
    "href": "05-PO11Q.html#first-steps-in-r",
    "title": "Week 5",
    "section": "First Steps in R",
    "text": "First Steps in R\nBut enough of the preliminary talk, let’s get started in R. In principle, you can think of R as a massive and powerful calculator. So I will use it as such to start of with. If you want to know what the sum of 5 and 3 is, you type (in the RScript, not the Console):\n\n5+3\n\nand press “command” / “enter” (or “Ctrl” / “enter” if you are on Windows). In everything that is to follow, commands will be shown in their own individual boxes. These have a clipboard button on the right, so you can copy and paste the code into your own RScript. The output is presented in a separate box directly underneath. There is no clipboard button, as R will render this result in your own console when you run the code. So, including the result, the calculation would look like this:\n\n5+3\n\n[1] 8\n\n\nwhere the [1] indicates that the 8 is the first component of the result. In this case, we only have one component, so it’s superfluous really, but we will soon encounter situations in which results can have a number of different items. Note that the result of this operation is displayed in the “Console”, even if you write this in the RScript above.\n\nYou can copy the code from this page by clicking the clipboard in the top-right hand corner. You can then paste it into your RScript.\n\nA fundamental component of R is objects. You can define an object by way of a reversed arrow, and you can assign values, characters, or functions to them. If we want to assign the sum of 5 and 3 to an object called result, for example, we call4\n\nresult &lt;- 5+3\n\nIf we now call the object, R will return its value, 8.\n\nresult\n\n[1] 8\n\n\n\nMake a habit of adding a note underneath each code chunk in your RScript (preceded with a #) in which you translate the code into plain English. This is especially useful for the lengthy complex chunks.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#the-working-directory",
    "href": "05-PO11Q.html#the-working-directory",
    "title": "Week 5",
    "section": "The Working Directory",
    "text": "The Working Directory\nIt is imperative that you create a suitable filing system to organise the materials for all of your modules. At the very least you should have a folder called “University” or similar, in which you have a sub-folder for each module you take.\nIn those modules in which you are working with R, you need to extend this system a little. I have created a schematic of what I have in mind in Figure 4.\n\n\n\n\n\n\nFigure 4: Folder Structure\n\n\n\nYou see that there is a sub-folder for each week of the module, and that each of these folders is divided into lecture and seminar in turn. Into these you can place the lecture and seminar materials, respectively. Create this system now for PO11Q.\nR works with so-called Working Directories. You can think of these as drawers from which R takes everything it needs to conduct the analysis (such as the data set), and into which it puts everything it produces (such as graph plots). As this will be an R-specific drawer within the seminar, create yet another sub-folder in your seminar folder, and call it something suitable, such as “PO11Q_Seminar_Week 1”. Do NOT call this “Working Directory”, as you will have many of those, rendering this name completely meaningless. Save the file EU.xlsx into this folder. Data are taken from European Comission (n.d.).\n\nPlease set up this structure now. If I find you using a random folder on your desktop named “working directory” in the coming weeks, I am going to implode! I mean it.\n\nNow we need to tell R to use this folder. If you know the file structure of your computer you can simply use the setwd() command, and enter the path. Here is an example from my computer:\n\nsetwd(\"~/Warwick/Modules/PO11Q/Seminars/Week 5/R Week 5\")\n\nIf you don’t know the file structure of your computer, then you can click Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory.\n\nWorking Directory",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#r-packages",
    "href": "05-PO11Q.html#r-packages",
    "title": "Week 5",
    "section": "R Packages",
    "text": "R Packages\nIt would be difficult to overstate the importance of packages in R. The program has a number of “base” functions which enable the user to do many different basic things, but packages are extensions that allow you to do pretty much anything and everything with this software - this is one of the reasons why I love it so much. The first package we need to use will enable us to load an Excel sheet into R. It is called readxl. You can install any package with the command install.packages() where the package name goes, wrapped in quotation marks, into the brackets:\n\ninstall.packages(\"readxl\")\n\nWe can then load this package into our library with the library() command.\n\nlibrary(readxl)\n\n\nOnce you close R at the end of a session, the library will be reset. When you reopen R, you have to load the packages you require again. But you do not have to install them again.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#opening-your-data-set",
    "href": "05-PO11Q.html#opening-your-data-set",
    "title": "Week 5",
    "section": "Opening your Data Set",
    "text": "Opening your Data Set\nWe are now ready to open a data set in R - where it is called a “data frame”. For this, we create a new object EU, and ask R to read “Sheet 1”” of the Excel file “EU.xlsx” which we placed in the working directory earlier\n\nEU &lt;- read_excel(\"EU.xlsx\", sheet=\"Sheet1\")\n\nWe can now use our data in R!\n\nLoading the Data Set\n\n\n\n\nPlease do not use the “Import Dataset” button in the Environment, but do this properly, manually. We sometimes need to set options for importing data sets, and the “pointy, clicky” approach won’t be able to offer you what you need.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#viewing-the-data",
    "href": "05-PO11Q.html#viewing-the-data",
    "title": "Week 5",
    "section": "Viewing the Data",
    "text": "Viewing the Data\nUnless you have been cheeky and opened the file in Excel to have a look, you have no idea yet, what the data look like. So it’s a good idea to view the data frame before doing anything with it. You can use the View() command to see the data frame:\n\nView(EU)\n\nIf you only want to see the first 6 observations of each variable, use the head() command:\n\nhead(EU)\n\n# A tibble: 6 × 5\n  country     pop18 access   area GDP_2015\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 Belgium  11413058   1951  30280  4.66e11\n2 Bulgaria  7050034   2007 108560  1.22e11\n3 Czechia  10610055   2004  77230  3.19e11\n4 Denmark   5781190   1973  42430  2.46e11\n5 Germany  82850000   1951 348540  3.60e12\n6 Estonia   1319133   2004  42390  3.51e10\n\n\nIf you simply want to know the variable names in the data frame, type:\n\nnames(EU)\n\n[1] \"country\"  \"pop18\"    \"access\"   \"area\"     \"GDP_2015\"\n\n\nThe next one is a very important command, because it reveals not only the variable names and their first few observations, but also the nature of each variable (numerical, character, etc.). It is the str() command, where “str” stands for structure:\n\nstr(EU)\n\ntibble [28 × 5] (S3: tbl_df/tbl/data.frame)\n $ country : chr [1:28] \"Belgium\" \"Bulgaria\" \"Czechia\" \"Denmark\" ...\n $ pop18   : num [1:28] 11413058 7050034 10610055 5781190 82850000 ...\n $ access  : num [1:28] 1951 2007 2004 1973 1951 ...\n $ area    : num [1:28] 30280 108560 77230 42430 348540 ...\n $ GDP_2015: num [1:28] 4.66e+11 1.22e+11 3.19e+11 2.46e+11 3.60e+12 ...\n\n\nYou can see that R has recognised most variables as numerical, one is displayed as a character variable. This is appropriate for some variables, such as pop18, but not for the ordinal variable access which is ordinal. We need to recode it, and all other variables we are unhappy with.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#variable-types-in-r",
    "href": "05-PO11Q.html#variable-types-in-r",
    "title": "Week 5",
    "section": "Variable Types in R",
    "text": "Variable Types in R\nR distinguishes between a number of different variable types and here is a broad overview of them. This will help you in deciding which descriptive statistics to calculate, or into which variable type you need to recode (next step) to achieve what you want. There are two general types:\n\nnumeric – numbers\ncharacter (also called string) – letters\n\nWithin numeric we can distinguish between the following:\n\nfactor - nominal\nordered factor - ordinal\ninteger - numeric, but only “whole” numbers (discrete)\nnumeric - any number (interval or ratio)\n\nNumerical variables are already in the data set, we have to attend to nominal and ordinal variables.\n\nNominal Variables\nIn terms of the variable types we encountered in the lecture this week, the country name is a nominal variable. So we need to tell R to turn this into a factor variable. We do this as follows:\n\nEU$country = factor(EU$country)\n\n\n\nOrdinal Variables\nAs mentioned above, the variable access should be ordinal, and therefore has to be turned into an ordered factor. The command which follows is almost identical to producing a factor variable, only that we add the option ordered = TRUE at the end:\n\nEU$access_fac = factor(EU$access, ordered = TRUE)\n\nIf you are familiar with European Studies, you will know that each accession wave has got a particular name. The 1973 enlargement, for example, is called the “First Enlargement”, the 1981 wave the Mediterranean Enlargement, and so forth. Let us create a new variable which uses these names instead of the years.\nThis process is a little more involved, and requires a new package to be installed and loaded: dplyr. This package is part of the so-called tidyverse which is a suite of packages designed to make working with R simpler and commands shorter. You can install all of them by calling install.packages(\"tidyverse\"). We then load the tidyverse with:\n\nlibrary(tidyverse)\n\nThe command which follows takes a little explaining. We start by stating the dataframe we wish to assign the result to, EU. Then we name the data frame that contains the data we wish to manipulate, here also EU. The symbol which follows, \\%&gt;\\%, reads as “and then”, and is called a “pipe”. So we take the data frame EU “and then” carry out a function called mutate. It creates a new variable called wave by recoding the variable access_fac. The command then specifies all categories of the “old” variable access_fac and what their respective values in the “new” variable wave are going to be. The categories in each are set in quotation marks, as they are factor / character categories.\n\nEU &lt;- EU %&gt;%\n  mutate(wave = recode(access_fac, '1951'=\"Founding\", \n                       '1973'= \"First\",\n                       '1981'= \"Mediterranean\",\n                       '1986' = \"Mediterranean\",\n                       '1995' = \"Cold War\",\n                       '2004' = \"Eastern\",\n                       '2007' = \"Eastern\",\n                       '2013' = \"Balkans\"))\n\nPlease note that some colleagues in the department object to the use of the tidyverse as a “dialect” of R, and require you to use base R in their modules. However, on this module I am still using the tidyverse, as:\n\nmy textbook, which is going to be the main textbook for this module from 2026, uses the tidyverse, and I think it is pedagogically wrong to divert from the main text in my seminars\nI do not think it makes sense to exclude one of currently 22,820 packages5\nGGPLOT2 which is part of the tidyverse simplifies code for generating figures significantly and will do for all but the most specific requirements\na lot of support on stackexchange is geared toward the tidyverse as a lot of US-based data scientists work with this package, and so you will find it easier to solve problems\n\nBut to keep everybody happy, I am providing the base R code whenever possible in a collapsible section like this one:\n\n\n\nBase R Solution\n\n\nEU$wave &lt;- NA\nEU$wave[EU$access_fac=='1951'] &lt;- \"Founding\"\nEU$wave[EU$access_fac=='1973'] &lt;- \"First\"\nEU$wave[EU$access_fac=='1981'] &lt;- \"Mediterranean\"\nEU$wave[EU$access_fac=='1986'] &lt;- \"Mediterranean\"\nEU$wave[EU$access_fac=='1995'] &lt;- \"Cold War\"\nEU$wave[EU$access_fac=='2004'] &lt;- \"Eastern\"\nEU$wave[EU$access_fac=='2007'] &lt;- \"Eastern\"\nEU$wave[EU$access_fac=='2013'] &lt;- \"Balkans\"\n\nEU$wave &lt;- factor(EU$wave, ordered = TRUE)\n\nHere, we first create a new, empty variable called wave in the EU data set. We then create new values , for example Founding in the variable EU$wave for the condition (this is what the square brackets [ ] do) that the variable access_fac in the EU data set, equals a specific value. For Founding this is is 1951. The last step is to turn the wave variable into an ordered factor.\n\n\nBut back to the recoding exercise itself. Please note that the original variable access_fac was already an ordered factor. Therefore, R (or the mutate function to be precise) also returns wave as an ordered factor. Had access_fac been an unordered factor (aka nominal variable), wave would also have been an unorderd factor. You can specify in an option to the mutate function whether you want the factor to be ordered or not:\n\nEU &lt;- EU %&gt;%\n  mutate(wave = recode(access_fac, '1951'=\"Founding\", \n                       '1973'= \"First\",\n                       '1981'= \"Mediterranean\",\n                       '1986' = \"Mediterranean\",\n                       '1995' = \"Cold War\",\n                       '2004' = \"Eastern\",\n                       '2007' = \"Eastern\",\n                       '2013' = \"Balkans\"), ordered=TRUE)\n\naccess only had a handful of numbers, and it turned out that each numerical value of this variable turned into a category in the new one. But often you have to recode variables of a more continuous nature into categories. Suppose, for example, we have the variable age which contains the age of respondents in a survey. If we wanted to recode this into categories such as 20-25, 26-35, etc., it would be tedious to assign a category to each individual value of age. This is where the cut() function comes in handy, as it literally cuts up a variable into chunks at the points we specify. Let’s apply this to the access variable.\nAgain, we use the mutate function, this time naming our new variable wave1 (so as not to overwrite the wave variable we created with the recode() function). The cut() function splits a numeric variable into intervals using the breaks we specify in the function. Importantly, these intervals are left-open and right-closed. This means that each interval includes its upper boundary, but not the lower one. This sounds complicated, so let me give you an example. If, for example, we called\n\ncut(x, breaks = c(10, 20, 30))\n\nthen the first interval would start at 10 and include all values greater than 10 and up to and including 20. The next interval would contain all values that are larger than 20 and up to and including 30.\nWhen we apply this to our access variable, we therefore need to specify the breaks as follows:\n\ncut(access, breaks=c(1950, 1951, 1973, 1986, 1995, 2007, 2013))\n\nThe first category (Founding) thus contains all years up to and including 1951, the next category (First), all years larger than 1951 and up to and including 1973, and so forth. Having created these categories (or levels in R terminology), we can then label them accordingly with labels.\n\nEU &lt;- EU %&gt;% \n  mutate(wave1=cut(access, \n                  breaks=c(1950, 1951, 1973, 1986, 1995, 2007, 2013), \n                  labels=c(\"Founding\",\"First\",\n                           \"Mediterranean\", \n                           \"Cold War\", \n                           \"Eastern\", \n                           \"Balkans\"))) \n\n\nlevels(EU$wave)\n\n[1] \"Founding\"      \"First\"         \"Mediterranean\" \"Cold War\"     \n[5] \"Eastern\"       \"Balkans\"      \n\n\n\n\n\nBase R Solution\n\n\nEU$wave &lt;- cut(EU$access, \n                  breaks=c(1950, 1951, 1973, 1986, 1995, 2007, 2013), \n                  labels=c(\"Founding\",\"First\",\n                           \"Mediterranean\", \n                           \"Cold War\", \n                           \"Eastern\", \n                           \"Balkans\"))\n\n\n\n\nRecoding a Factor Variable\n\n\n\n\nRecoding Ordered Factor Variables\n\n\n\n\n\nBinary Dummy\nVery often in political science we have yes/no scenarios, such as democracy yes or no, civil war, yes or no, etc. To analyse these scenarios, we can create so-called “dummy variables”. In the present example, let’s specify for each country whether it has been a founding member of the EU. It is a factor variable and so we do could do this exactly the same way as our initial recoding of the wave variable above:\n\nEU &lt;- EU %&gt;%\n  mutate(founding = recode(access_fac, '1951'=\"Yes\", \n                       '1973' = \"No\",\n                       '1981' = \"No\",\n                       '1986' = \"No\",\n                       '1995' = \"No\",\n                       '2004' = \"No\",\n                       '2007' = \"No\",\n                       '2013' = \"No\"))\n\nThere is a much shorter way to do this, however, and shorter is always preferred in coding so long as it leads to the same result. We can apply the ifelse() function which follows the rationale: ifelse('condition', 'if condition met, then', 'otherwise'). Here, if access_fac==\"1951\" we want to assign the value \"Yes\", if not \"No\":\n\nEU &lt;- EU %&gt;% \n   mutate(founding = factor(ifelse(access_fac==\"1951\", \"Yes\", \"No\"), \n                              levels =c(\"Yes\", \"No\")))\n\n\n\n\nBase R Solution\n\nUsing ifelse, this is very similar, only the pipe disappears:\n\nEU$founding &lt;- factor(ifelse(EU$access_fac==\"1951\", \"Yes\", \"No\"), \n                              levels =c(\"Yes\", \"No\"))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#sub-setting-data",
    "href": "05-PO11Q.html#sub-setting-data",
    "title": "Week 5",
    "section": "Sub-Setting Data",
    "text": "Sub-Setting Data\nWhen we start analysing data, we rarely need all data at the same time. We might not need some variables, at all, for example, or we only want to work with certain observations, such as those countries in the “founding” wave. In these cases, we can subset the data. I will show you some examples of subsetting now.\n\nBy Variable\nIf you are sure you won’t need a variable (remember, there is no back button), you can simply drop (i.e. delete) it. Let’s do this with the area variable:\n\nEU$area &lt;- NULL\n\nIf we are dropping multiple variables, we can either perform this operation each time, or use another command which allows us to operate with multiple variables at the same time. The select() command comes from the tidyverse package and specifies which variables we wish to keep:\n\nEU_pop &lt;- select(EU, country, pop18, access_fac, founding)\n\nThis creates a new data frame called EU_pop containing only the variables country, pop18, access_fac, and founding.\n\n\n\nBase R Solution\n\nThe package documentation offers some basic instructions how to convert the tidyverse (or dyplyr, to be precise) code into base R. But here is the solution for the previous code chunk:\n\nEU_pop &lt;- subset(EU, country, pop18, access_fac, founding)\n\n\n\nWe can, however, use the same command and tell R which variables to drop by adding a minus sign in from of the variables we want to delete. The following command produces exactly the same result as the one before:\n\nEU_pop1 &lt;- select(EU, -access, -GDP_2015)\n\n\n\nBy Observation\nInstead of dropping and keeping variables, we can do the same thing to individual observations. Here, we use the slice() command (like a cake) and specify which slices we want to drop or keep. For example to drop the Benelux countries we would delete observations 1, 16 and 19:\n\nEU_nobenelux &lt;- slice(EU, -1, -16, -19)\n\n\n\n\nBase R Solution\n\n\nEU_pop &lt;- EU[c(-1, -16, -19),]\n\n\n\nAlternatively, if we were only interested in Benelux countries we would subset to only those observations:\n\nEU_benelux &lt;- slice(EU, 1, 16, 19)\n\n\n\n\nBase R Solution\n\n\nEU_pop &lt;- EU[c(1, 16, 19),]\n\n\n\n\n\nKeep if a variable has a certain value\nOne of the most useful commands is filter(), as it allows us to keep all observations for which the value of a variable is of a particular number. For example if we wanted to conduct an analysis with all countries which have a population in excess of 10 million we could subset by:\n\nEU_pop_large &lt;- filter(EU, pop18 &gt; 10000000)\n\n\n\n\nBase R Solution\n\n\nEU_pop_large &lt;- subset(EU, pop18 &gt; 10000000)\n\n\n\nHere is a list of some operators you can use for this purpose:\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\nDescription\n\n\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to\n\n\n!x\nNot x\n\n\nx | y\nx OR y\n\n\nx & y\nx AND y\n\n\n\n\n\n\nTable 1: Operators in R\n\n\n\n\n\n\n \n\nSubsetting Data",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#ordering-data",
    "href": "05-PO11Q.html#ordering-data",
    "title": "Week 5",
    "section": "Ordering Data",
    "text": "Ordering Data\nThe data set in its original state is purposely not ordered by any criterion, such as alphabetical order of countries, etc. But we can use R to do exactly that. Let us work with a subset containing only three variables:\n\nEU_subset &lt;- select(EU, country, pop18, access)\n\nIt would be lovely if the command for ordering data would be called order(), but it is called arrange()6. Let’s order countries by ascending population in a new data frame called eu_order:\n\n\n\nBase R Solution\n\n\neu_order &lt;- EU_subset[order(EU_subset$pop18),]\n\n\n\nWe can now display the first 10 rows with the following command:\n\neu_order[1:10,]\n\n# A tibble: 10 × 3\n   country      pop18 access\n   &lt;fct&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1 Malta       475701   2004\n 2 Luxembourg  602005   1951\n 3 Cyprus      864236   2004\n 4 Estonia    1319133   2004\n 5 Latvia     1934379   2004\n 6 Slovenia   2066880   2004\n 7 Lithuania  2808901   2004\n 8 Croatia    4105493   2013\n 9 Ireland    4838259   1973\n10 Slovakia   5443120   2004\n\n\nThe content in the brackets refers to the rows (before the comma), and to the columns (after the comma). As we only want certain rows and displaying all variables, I have left the space after the comma blank.\nWe can do the same thing in descending order by calling:\n\neu_order &lt;- arrange(EU_subset, desc(pop18))\neu_order[1:10,]\n\n# A tibble: 10 × 3\n   country           pop18 access\n   &lt;fct&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n 1 Germany        82850000   1951\n 2 France         67221943   1951\n 3 United Kingdom 66238007   1973\n 4 Italy          60483973   1951\n 5 Spain          46659302   1986\n 6 Poland         37976687   2004\n 7 Romania        19523621   2007\n 8 Netherlands    17181084   1951\n 9 Belgium        11413058   1951\n10 Greece         10738868   1981\n\n\n\n\n\nBase R Solution\n\n\neu_order &lt;- EU_subset[order(desc(EU_subset$pop18)),]\n\n\n\nA neat feature of R is that it allows us to order observations by more than one variable. So for example, we could order them by ascending accession wave first, and then by ascending population in 2018 as follows:\n\neu_order &lt;- arrange(EU_subset, access, pop18)\n\neu_order[1:10,]\n\n# A tibble: 10 × 3\n   country           pop18 access\n   &lt;fct&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n 1 Luxembourg       602005   1951\n 2 Belgium        11413058   1951\n 3 Netherlands    17181084   1951\n 4 Italy          60483973   1951\n 5 France         67221943   1951\n 6 Germany        82850000   1951\n 7 Ireland         4838259   1973\n 8 Denmark         5781190   1973\n 9 United Kingdom 66238007   1973\n10 Greece         10738868   1981\n\n\n\n\n\nBase R Solution\n\n\neu_order &lt;- EU_subset[order(EU_subset$access,EU_subset$pop18),]\n\n# or slightly shorter \n\neu_order &lt;- EU_subset[order(with(EU_subset, access,pop18)),]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#grouping-data",
    "href": "05-PO11Q.html#grouping-data",
    "title": "Week 5",
    "section": "Grouping Data",
    "text": "Grouping Data\nLooking at the last example, a question that might spring up is in which accession wave the joining countries brought the largest population increase on average to the EU. We can calculate summary statistics for a particular group by, well, grouping them. The first step is to group data into rows with the same value:\n\neu_access &lt;- group_by(EU_subset, access)\n\nBy the way: whenever you have grouped anything, and finished analysing data in this grouped version it is essential that you ungroup the data afterwards, so that you don’t unintentionally keep using the groups:\n\nungroup(EU_subset)\n\n# A tibble: 28 × 3\n   country     pop18 access\n   &lt;fct&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 Belgium  11413058   1951\n 2 Bulgaria  7050034   2007\n 3 Czechia  10610055   2004\n 4 Denmark   5781190   1973\n 5 Germany  82850000   1951\n 6 Estonia   1319133   2004\n 7 Ireland   4838259   1973\n 8 Greece   10738868   1981\n 9 Spain    46659302   1986\n10 France   67221943   1951\n# ℹ 18 more rows\n\n\nBut let’s calculate the average population size per accession wave in an elegant command which combines multiple steps by using pipes:\n\neu_popaccess &lt;- EU_subset %&gt;% \n  group_by(access) %&gt;% \n  summarise(avg = mean(pop18))\n\neu_popaccess\n\n# A tibble: 8 × 2\n  access       avg\n   &lt;dbl&gt;     &lt;dbl&gt;\n1   1951 39958677.\n2   1973 25619152 \n3   1981 10738868 \n4   1986 28475164.\n5   1995  8151880.\n6   2004  7327746.\n7   2007 13286828.\n8   2013  4105493 \n\n\n\n\n\nBase R Solution\n\n\neu_popaccess1 &lt;- aggregate(pop18 ~ access, \n                           data = EU_subset, \n                           FUN = mean )\n\n\n\nYou now see a new variable called avg which contains the average population increase for each wave. In which wave did the joining countries have the largest population on average?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#combining-ordering-and-grouping-data",
    "href": "05-PO11Q.html#combining-ordering-and-grouping-data",
    "title": "Week 5",
    "section": "Combining Ordering and Grouping Data",
    "text": "Combining Ordering and Grouping Data\nThe question was easy to answer here, as we only have a few accession waves. It starts to get unwieldy though, the more groups we have, but we can let R do the job by combining first grouping, and then ordering. So we take the grouped data frame eu_popaccess and order it by descending avg:\n\neu_popaccess_order &lt;- arrange(eu_popaccess, desc(avg))\n\neu_popaccess_order\n\n# A tibble: 8 × 2\n  access       avg\n   &lt;dbl&gt;     &lt;dbl&gt;\n1   1951 39958677.\n2   1986 28475164.\n3   1973 25619152 \n4   2007 13286828.\n5   1981 10738868 \n6   1995  8151880.\n7   2004  7327746.\n8   2013  4105493 \n\n\n\n\n\nBase R Solution\n\n\neu_popaccess_order &lt;- eu_popaccess[order(desc(eu_popaccess$avg)),]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#saving",
    "href": "05-PO11Q.html#saving",
    "title": "Week 5",
    "section": "Saving",
    "text": "Saving\nPlease now save this RScript into the same folder (working directory) as the raw data. When R asks before closing, there is no need to save the workspace or the data, as running the RScript on the raw data will bring you precisely to where you left off.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO11Q.html#footnotes",
    "href": "05-PO11Q.html#footnotes",
    "title": "Week 5",
    "section": "",
    "text": "Some of the content of this worksheet is taken from Linke (forthcoming).↩︎\nSource: https://www.garrickadenbuie.com/project/rsthemes/↩︎\nThis is a variation of the Dracula Theme.↩︎\nTo “call” means to execute a command.↩︎\nhttps://cran.r-project.org/web/packages/ updated 02 October 2025↩︎\nThere is a command called order(), but it is not part of the tidyverse, and as this package is steadily on the rise in coding, I am only showing you this here.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "06-PO11Q.html",
    "href": "06-PO11Q.html",
    "title": "Week 6",
    "section": "",
    "text": "Reading Week\nReading week is not an institutionalised holiday, and I do expect you to put in about 10 hours of work for this module over the course of this week. Here are a few suggestions for activities to fill these 10 hours:\n\nCatch up on the reading\nRevise the material of Weeks 1-5, as the going will get a little tougher in Week 7.\nGo through the worksheets of Weeks 5 to make sure you are on top of things with R.\nRevise all R functions up to this point. You can use the Week 5 Flashcard Section for this.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html",
    "href": "07-PO11Q.html",
    "title": "Week 7",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#how-to-read-a-z-table",
    "href": "07-PO11Q.html#how-to-read-a-z-table",
    "title": "Week 7",
    "section": "How to read a z-table",
    "text": "How to read a z-table\nIn the lecture you have learned about the Normal Distribution. Under the Normal Distribution, the area under the curve is determined by the number of standard deviations around our mean \\(\\mu\\). This number is expressed in the form of the z-score which is defined as:\n\\[\\begin{equation}\nz=\\frac{\\text{Observation} - \\text{Mean}}{\\text{Standard Deviation}}=\\frac{y-\\mu}{\\sigma}\n\\end{equation}\\]\nTo put this in words, z takes the difference between a particular value we are interested in and the mean. It then divides this distance by the standard deviation, in order to express the distance in units of standard deviations. Why do we do this? We know that under the Normal Distribution the area of the interval mean \\(\\pm\\) one standard deviation is equal to 68%. This is equivalent to the blue area in Figure 1. This also means that the remaining white area is equal to 32%, or the white section on each side 16%.\n\n\n\n\n\n\nFigure 1: Area under the Normal Distribution\n\n\n\nImagine now, we took the point of minus one standard deviation as a starting point, and turn right, as in Figure 2. The white area is still 16%, so that the blue area needs to be 84%.\n\n\n\n\n\n\nFigure 2: Right-Tail Probability\n\n\n\nSo the probability of finding a value larger than what is equivalent to minus one standard deviation is 84%. We call this a right-tail probability. The beauty is that we can do this for any point on the x-axis. Once we know how many standard deviations a value is removed from the mean, we can use the right-tail probability to assess how likely a value higher (or lower) than this value is to occur.\nThe number of standard deviations is the z-score. Every z-score has a right-tail probability associated with it. These probabilities are listed in the Normal Table. How do we read this Table? Let me take you through the example used in the lecture once more. The crime rate in our 32 Scottish councils is normally distributed, with \\(\\mu=280.36\\) and \\(\\sigma=69.26\\). You can see this distribution visualised in Figure 3.\n\n\n\n\n\n\nFigure 3: Distribution of Crime Rate in 32 Scotting Councils, 2020\n\n\n\nThe question then was how likely it was for us to find a council with a crime rate larger than 400. If we wanted to visualise this, we would need the area to the right of 400 on the x-axis. This would look like this:\n\n\n\n\n\n\nFigure 4: Probability of a Council with a Crime Rate \\(\\geq\\) 400\n\n\n\nIn order to calculate the size of this area, we first took the difference between 400 and 280.36 which is 119.64. We then divided 119.64 by the standard deviation of 69.26, to express the distance in units of the standard deviation. The result is 1.727404. We know, therefore, that a crime rate of 400 on the x-axis is located 1.727404 standard deviations to the right of the mean.\nWe now need to find the right-tail probability that belongs to this value. In the left-most column of the Normal Table you find the z-values with the first decimal place. Move down to 1.7. From here you turn right, until you hit the second decimal place. As our value is 1.73 you will have to go four columns to the right (the first one is for decimal 0). For a z-score of 1.73 the area is 0.0418, or 4.2%. We can therefore say that with an average crime rate of 280.36 and a standard deviation of 69.26, the probability of finding a council with a crime rate higher than 400 was 4.2%.\nBefore moving on, I need to note that z can be negative. If we were assessing the probability of finding a council with a crime rate of less than 160.5 (280.36 - 1.73*69.26) we would get a z-score of -1.73. Because the Normal Distribution is symmetrical, we can use the same process, but need to reverse the logic. Because the right tail probability gives us the area to the right of the z-score, a negative z-score would give us the area to the left of the z-score. So, the probability of finding a council with a crime rate of less than 160.5 is also 4.2%.\nWith this knowledge at hand, let’s do some calculations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#calculations",
    "href": "07-PO11Q.html#calculations",
    "title": "Week 7",
    "section": "Calculations2",
    "text": "Calculations2\n\nThe mean weight of a bag of apples is 1 kg. The weight of bags is normally distributed around this mean with a standard deviation of 50g.\n\nBilly is looking for the heaviest bag possible and finds one that is 1082 g. What is the probability of finding a heavier bag?\nWhat is the probability that Billy will find a bag lighter than 870g?\nHow would the results of a. and b. change if the standard deviation was only 40g? Why?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#solutions",
    "href": "07-PO11Q.html#solutions",
    "title": "Week 7",
    "section": "Solutions",
    "text": "Solutions\nYou can find the Solutions in the Downloads Section.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#preliminary-stuff",
    "href": "07-PO11Q.html#preliminary-stuff",
    "title": "Week 7",
    "section": "Preliminary Stuff",
    "text": "Preliminary Stuff\nCreate a working directory (folder) for Week 7, create an RScript for Week 7, and set the working directory. In my case:\n\nsetwd(\"~/Warwick/Modules/PO11Q/Seminars/Week 7/Worksheet\")\n\nLoad the data set Example.xlsx (taken from European Comission (n.d.)) into R:\n\nlibrary(readxl)\n\nEU &lt;- read_excel(\"EU.xlsx\", sheet=\"Sheet1\")\n\nNow copy/paste the entire RScript from Week 5 into the Week 7 RScript. Run the entire script by highlighting everything (“command”/“a” on a Mac and “ctrl”/“a” on Windows) and executing. You should now have a completely and correctly coded data frame that is ready to work with in your R Workspace. If not, give me a shout to sort out problems.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#descriptive-statistics",
    "href": "07-PO11Q.html#descriptive-statistics",
    "title": "Week 7",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nWe have covered quite a large number of descriptive statistics, so far. These are:\n\nMean\nMedian\n\nMode\nStandard Deviation\nVariance\nQuartiles and Percentiles\nRange\nInterquartile Range\n\nThey are a lot of effort to calculate by hand, especially for larger data sets, but R can do these with a few intuitive commands. First up is the mean.\n\nmean(EU$pop18)\n\n[1] 18311106\n\n\nThen the median:\n\nmedian(EU$pop18)\n\n[1] 9300319\n\n\nAs a reminder from Week 5, you can adjust mean, median, and mode for the following distribution, and see how it changes:\nYou can get information on the quartiles (remember that the median is the second quartile), the mean, as well as the minimum and maximum through one, simple command:\n\nsummary(EU$pop18)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  475701  3781345  9300319 18311106 17766718 82850000 \n\n\nIf you want a specific percentile, say the 10\\(^\\text{th}\\), you can use the quantile() function:\n\nquantile(EU$pop18, p=0.1)\n\n    10% \n1182664 \n\n\nAnd this brings us neatly to measures of variability. Next up is the range; you can either calculate this with two commands by finding out the minimum and maximum separately, or just ask R to give you both values straight away:\n\nmin(EU$pop18)\n\n[1] 475701\n\nmax(EU$pop18)\n\n[1] 82850000\n\nrange(EU$pop18)\n\n[1]   475701 82850000\n\n\nThe stadard deviation is rather long-winded to calculate by hand, but the R command is short and sweet:\n\nsd(EU$pop18)\n\n[1] 23787945\n\n\nAs you know, the variance is the squared standard deviation, but you can calculate it with its own command in R, too:\n\nvar(EU$pop18)\n\n[1] 5.658663e+14\n\n\n\nDescriptive Statistics\n\n\n\nNow that you have all the relevant tools at hand, complete the following tasks:\n\nGenerate descriptive statistics for 3 of our variables.\nRecode the variable ‘GDP_2015’ into a ordered factor called ‘gdp_level’ with three levels called “low”, “medium”, and “high” with cut-off points of your own choosing.\nProduce a tabulation for ‘gdp_level’.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#graphs",
    "href": "07-PO11Q.html#graphs",
    "title": "Week 7",
    "section": "Graphs",
    "text": "Graphs\nR is probably the most powerful statistics programme for creating graphs. As this is an introductory level module, and we only have so much time available in the seminars, I will only be able to introduce you to the most commonly used ones; in the first instance histograms and boxplots. I will then introduce you to the package ggplot2 which is simply the best invention since sliced bread, as it gives you pretty much endless optionality in customising graphs to show exactly what you want.\nWhenever you produce a graph and you use it in an essay, your dissertation, or article, it is crucial that the graph is able to communicate its message independently from the text. So, a reader should be able to understand the graph and be able to appreciate fully its message without having to read the text. In a similar fashion, the text should always be written in such a way that a reader is able to understand it without having to look at the graph. This is a principle which equally applies to tables (more on this on PO12Q). If you do not follow this principle in the assessments on my modules, you will be marked down.\nChapter 4 in “The Visual Display of Quantitative Information” by Tufte (2001) is on the reading list as an essential item, but there are some more principles he sets out at the start of the book (p. 13) which are worthwhile repeating here:\n\nExcellence in statistical graphics consists of complex ideas communicated with clarity, precision, and efficiency. Graphical displays should\n\n\n\nshow the data\ninduce the viewer to think about the substance rather than about methodology, graphic design, the technology of graphic production, or something else\navoid distorting what the data have to say\npresent many numbers in a small space\nmake large data sets coherent\nencourage the eye to compare different pieces of data\nreveal the data at several levels of detail, from a broad overview to the fine structure\nserve a reasonably clear purpose: description, exploration, tabulation, or decoration\nbe closely integrated with the statistical and verbal descriptions of a data set.\n\n\n\nGraphics reveal data. Indeed graphics can be more precise and revealing than conventional statistical computations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#basic-graphs",
    "href": "07-PO11Q.html#basic-graphs",
    "title": "Week 7",
    "section": "Basic Graphs",
    "text": "Basic Graphs\nLet’s start with a histogram of the variable pop18. The range of the pop18 variable is about 82 million - this is rather unwieldy to imagine and also to put onto axes of graphs, as they would mostly consist of zeros. So let’s express the population of each countries in million instead:\n\nEU$popmio &lt;- EU$pop18/1000000\n\nWe can now produce a histogram. Before doing this, it is sensible to think about the number of bars we want in the histogram. The smallest country has just shy of 500,000 inhabitants, whereas the largest has over 82 million. So, I would like the x-axis to run from zero to 100 (million) and divide this into 5 bars. Accordingly, we are introducing 4 breaks on the x-axis with the following command:\n\nhist(EU$popmio, breaks = 4)\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nThis is certainly a histogram, but it does not conform to the principle of graphs that they should be able to communicate their message independently, yet. Take the label of the x-axis, for example, what does EU$popmio mean? You and I know, but somebody who doesn’t know R language wouldn’t. We can tell R to adjust the axis label, as well as the main title of the histogram as follows:\n\nhist(EU$popmio, breaks = 4,\n     xlab = \"Population in million\",\n     main = \"Histogram of EU Population (2018)\")\n\n\n\n\n\n\n\nFigure 6: Histogram of EU Population (2018)\n\n\n\n\n\n\nHistograms\n\n\n\nThis is fine now. Ugly, but fine. I will show you how to do a boxplot next, and then I will take you through the process of making all this look a bit more jazzy. The command for the boxplot is very intuitive. The default in R is to arrange the boxplot vertically. I prefer them horizontally, and you can set this in an equally intuitive option.\n\nboxplot(EU$popmio, horizontal = TRUE)\n\n\n\n\n\n\n\nFigure 7: Boxplot of EU Population\n\n\n\n\n\nYou will recognise the descriptives we calculated earlier with the summary() function:\n\nsummary(EU$popmio)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4757  3.7813  9.3003 18.3111 17.7667 82.8500 \n\n\n\nExplain the outliers on the right mathematically.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#advanced-graphs",
    "href": "07-PO11Q.html#advanced-graphs",
    "title": "Week 7",
    "section": "Advanced Graphs",
    "text": "Advanced Graphs\nThe graphs we have produced so far are functional, but let’s be honest, they wouldn’t win any beauty contests. There is, as mentioned earlier, an amazing package called ggplot2 which changes this dramatically. You have already installed it as a part of the tidyverse. Otherwise, the function would be:\n\ninstall.packages(\"ggplot2\")\n\nWe can just load it:\n\nlibrary(ggplot2)\n\nThe “gg” in ggplot2 stands for “grammar of graphs”. You will be familiar with the term “grammar”” from learning a language already. In this context, we use grammar to build sentences by choosing and arranging a variety different components, such as subjects, verbs and objects. If you know how to do this properly, you can express exactly what you want to say. The grammar of graphs adopts this logic and specifies a number of different components which allow you to create a graph which is able to communicate exactly what you wish to show.\nggplot2 has eight basic grammatical arguments:\n \n\n\n\n\n\n\nData Frame\nThe data you wish to visualize.\n\n\nAesthetic Mappings\nHere you specify how the data are assigned to colour, size, etc. For now, this is the variable for which we want to create a graphical distribution.\n\n\nGeom\nShort for \"geometry\". Use a geom function to represent data points through geometric objects, such as points, lines, etc. Each function returns a layer.\n\n\nStat\nYou can include statistical summaries through this, such as smoothing, or regression lines.\n\n\nPosition\nPosition adjustments determine how to arrange geoms that would otherwise occupy the same space.\n\n\nFacets\nFacets divide a plot into sub-plots based on the values of one or more discrete variables.\n\n\nScale\nMaps data values to the visual values of an aesthetic. For example female=pink, male=blue.\n\n\nCoordinates\nHow do the numbers get translated onto the plot? We are not going to look at this on this module.\n\n\n\n\n\n\n \nI like to think of using these arguments like dressing myself in the morning. The minimum that common decency requires me to wear if I wish to leave the house is some underwear, some trousers, and a top. Depending on how I feel and what the weather is like, I can add more layers, like socks, a jumper, or a scarf. It is exactly the same with ggplot2. As a minimum to produce a plot you need a data frame, the aesthetic mapping and a geom. Once you have produced this minimalistic graph, you can modify it, by adding more components / arguments. As you can imagine the possibilities are almost endless, and we only have time to deal with the minimum here. This is not a problem, however, as most of the other grammatical arguments (Stats, Position, Facets and Scales) generally have sensible defaults.\nSo how does this work in practice? Let us reproduce the histogram of the age variable. We start by calling ggplot2 and advise the function which data frame we wish to use (EU). In a second step, we add a geometry – in our case geom_histogram. Within the geometry, we need to specify for which variable we wish to create a distribution, or in the language of ggplot2 which variable we wish to map to the geom as our Aesthetic. To produce 5 bars again, we specify a bandwith of 20 million (this refers to popmio).\n\nggplot(data = EU) +\n  geom_histogram(mapping = aes(popmio), binwidth = 20)\n\n\n\n\n\n\n\nFigure 8: Histogram of EU Population (binwidth = 20)\n\n\n\n\n\nAnnoyingly, ggplot places the axis ticks in the middle of each bar which is WRONG for histograms. They need to align with the boundaries of the bars. We do this by telling R the boundary of the plot:\n\nggplot(data = EU) +\n  geom_histogram(mapping = aes(popmio), binwidth = 20, \n                 boundary = 0)\n\n\n\n\n\n\n\nFigure 9: Histogram of EU Population (binwidth = 20, boundary = 0)\n\n\n\n\n\nThis has shifted the ticks to the left, but now R has decided to label the x-axis in steps of 25, whereas our bars have a bandwidth of 20. Once again, we have the variable name on the x-axis, instead of a label which anybody could understand. I also prefer “Frequency” on the y-axis, instead of “Count”. To address both of these concerns, we simply add a layer for each. First up are the axis ticks. Our variable is continuous, so we choose the scale_x_continuous option, and tell R to break the axis up into a sequence which starts with zero, ends at 100 and has steps of 20 in between. In the labs argument we adjust the labelling as intended:\n\nggplot(data=EU) +\n  geom_histogram(mapping=aes(popmio), binwidth = 20, \n                 boundary = 0) +\n  scale_x_continuous(breaks = seq(0, 100, 20)) +\n  labs(x=\"Population (in million)\", y=\"Frequency\") +\n  theme_classic()\n\n\n\n\nBase R Solution\n\n\nhist(EU$popmio, breaks = 4,\n     xlab = \"Population (in million)\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\n\nFigure 10: Histogram of EU Population with GGPLOT\n\n\n\nI have also removed the background in line with the principles set out by Tufte (2001, p. 96) by adding theme_classic(). This is it. A graph which can communicate its message independently, and which looks aesthetically pleasing.\nIn the present case we have the population, so displaying the frequency on the y-axis is sort of sensible, but usually we would be dealing with a sample. Here the count is not very telling and we would be using percentages, instead. Let’s do it!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#even-more-advanced-graphs",
    "href": "07-PO11Q.html#even-more-advanced-graphs",
    "title": "Week 7",
    "section": "Even More Advanced Graphs",
    "text": "Even More Advanced Graphs\nUnfortunately, there is no easy, default way to do this in R, but necessitates a calculation within the ggplot command. Once more we call ggplot and use the EU data set, and select the geom geom_histogram. Again we specify the binwidth as 20 with a boundary of zero, and put popmio on the x-axis. Now comes the point where we need to do something new, because y is not equivalent to the frequency any more, but should be percentage. To achieve this we advise R to put the density there (which is the relative frequency from the the lecture in week 5), and multiply this density by 100 to get percentage. Nothing has changed on the scaling of the x-axis from the previous plot, so we can copy and paste the scale_x_continuous section, as well as the labelling of the x-axis. In this last step, we now also need to adjust the label of the y-axis, because this has now percentage on it, and not frequency. The result is this:\n\nggplot(data = EU) + \n  geom_histogram(binwidth = 20, boundary = 0,\n                 aes(x= popmio, \n                     y = (..count..)/sum(..count..)*100)) +\n  scale_x_continuous(breaks = seq(0, 100, 20)) +\n  labs(x=\"Population (in million)\", y=\"percent\")  +\n  theme_classic()\n\n\n\n\nBase R Solution\n\n\n# Create histogram data without plotting\nh &lt;- hist(EU$popmio, \n         breaks = seq(0, 100, by = 20), \n         plot = FALSE)\n\n# Normalize counts to percentages\nh$counts &lt;- h$counts / sum(h$counts) * 100\n\n# Plot histogram\nplot(h, \n    freq = FALSE, \n    xlab = \"Population (in million)\", \n    ylab = \"percent\", \n    main = \"\")\n\n\n\n\n\n\n\n\n\nFigure 11: Histogram of EU Population with GGPLOT in Percent\n\n\n\n\nJazzy Graphs with GGPLOT",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#organising-code-in-the-rscript",
    "href": "07-PO11Q.html#organising-code-in-the-rscript",
    "title": "Week 7",
    "section": "Organising Code in the RScript",
    "text": "Organising Code in the RScript\nNow is probably a good time to make you aware of how I have been organising code which runs over several lines. I could also have written the code of the last graph as\n\nggplot(data = EU) + \n  geom_histogram(binwidth = 20, boundary = 0, aes(x= popmio, y = (..count..)/sum(..count..)*100)) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=\"Population\", y=\"percent\") + theme_classic()\n\nbut this would have made it rather difficult to disentangle and to spot the structure of the graph straight away. So it is also a good idea to structure the code in a logical way which allows a reader to understand it as easily as possible. R is very smart in the way it indents the next line after pressing “enter” in an RScript automatically to the appropriate position. You see for example that in\n\nggplot(data = EU) + \n  geom_histogram(binwidth = 20, boundary = 0,\n                 aes(x= popmio, \n                     y = (..count..)/sum(..count..)*100)) +\n  scale_x_continuous(breaks = seq(0, 100, 20)) +\n  labs(x=\"Population (in million)\", y=\"percent\") +\n  theme_classic()\n\nthe aes which belongs to the geom_histogram layer is indented just so it starts flush with the first argument (binwidth) within this layer.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#exercises",
    "href": "07-PO11Q.html#exercises",
    "title": "Week 7",
    "section": "Exercises",
    "text": "Exercises\nUsing these commands, and moving beyond with the help of today’s reading, complete the following tasks:\n\nProduce two base-R graphs of different types (e.g. histogram, bar chart, box-and-whisker plot) for separate variables in the EU data set.\nProduce two ggplot graphs of different types (e.g. histogram, bar chart, box-and-whisker plot) for separate variables in the EU data set. Google to find more geoms.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#captions-for-tables-and-figures-in-word",
    "href": "07-PO11Q.html#captions-for-tables-and-figures-in-word",
    "title": "Week 7",
    "section": "Captions for Tables and Figures in Word",
    "text": "Captions for Tables and Figures in Word\nIn essays, your dissertation and in articles, you will have to refer to tables and figures in the text. Now, you can do this by writing “the figure below”. But this is not very elegant. Also, what happens if you change the layout and all of a sudden “the figure below” becomes “the figure above”. This not only causes additional work because you have to edit the text and check all references to tables and figures once you are done (which is tedious beyond description), but there is also the risk that you miss one or a few in the process.\nMS Word has a nifty function that allows you to insert captions for figures and tables, and then to insert cross-references into the text which get updated automatically before you send the document to the printer. Here is how to do it:\nSay, you have a figure inserted into Word. You now click on it, then hover over the bottom right-hand square, and right-click with your mouse. From the resulting context menu you select “Insert Caption”.\n\n\n\nInsert caption\n\n\nThis results in the following window:\n\n\n\nCaption\n\n\nSelect whether the item you want to describe is a figure, or a table. Then make sure you place the caption “below” the item (this is default). Then type your caption into the box at the top, such as “Figure 1: Skewness of Distributions”. Make sure the caption is telling. The reader needs to know from the caption what the figure or table is about. When you click OK, the document looks like this:\n\n\n\n\n\nNow you start writing the text and come to the point where you refer to the figure in question. Here, all you have to do is to select “Insert” and “Cross-Reference”” \n\n\n\n\n\nand select the following options in the pop-up window:\n\n\n\n\n\n\nYour text will then look like this:\n\n\n\n\n\nYou don’t have to worry now about the sequence of numbering any more. If you insert another figure above this one, and insert a cross-reference in the text again, the sequence is automatically updated and our former “Figure 1” becomes “Figure 2”. Tables and figures have separate sequences of numbering.\nOne last word on the display of data in tables: DO NOT screenshot tables from R and insert them into your presentations. They look ugly and unprofessional. Make the effort and create a proper table, either in Word or Excel and populate it manually with the data from R. The insertion of captions and cross-references is the same as described above.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO11Q.html#footnotes",
    "href": "07-PO11Q.html#footnotes",
    "title": "Week 7",
    "section": "",
    "text": "Some of the content of this worksheet is taken from Linke (forthcoming).↩︎\nThese are taken from Linke (forthcoming).↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "08-PO11Q.html",
    "href": "08-PO11Q.html",
    "title": "Week 8",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO11Q.html#calculating-confidence-intervals",
    "href": "08-PO11Q.html#calculating-confidence-intervals",
    "title": "Week 8",
    "section": "Calculating Confidence Intervals",
    "text": "Calculating Confidence Intervals\nJust as a reminder, what is a confidence interval?\n\nConfidence Interval\nA confidence interval is an estimated range, based on a sample, that is likely to contain the true population parameter (such as the mean). If the sampling process were repeated many times, approximately ( (1 - ) % ) of the resulting intervals would contain the true parameter. The value of ( ) determines the confidence level – for example, ( = 0.05 ) corresponds to a 95% confidence level, while ( = 0.01 ) corresponds to 99%. (Linke, forthcoming)\n\nThere are two scenarios under which we are calculating confidence intervals: (1) we know \\(\\sigma\\) (the standard deviation of the population distribution), and (2) we don’t know \\(\\sigma\\). Let me take you through these two scenarios in turn:\n\n1. We know \\(\\sigma\\)\nIf the population distribution is known and the population standard deviation (\\(\\sigma\\)) is available, we can construct a confidence interval using the standard normal distribution (z-distribution).\nAssume we have a sample of student ages with sample size \\(n = 81\\) and a sample mean \\(\\bar{y} = 26\\). The population standard deviation is known to be \\(\\sigma = 9\\). We aim to construct a 99% confidence interval for the true population mean age of students. (In the lecture, we considered the 95% confidence level for this same scenario.)\nWe begin by calculating the standard error of the sample mean:\n\\[\\begin{equation*}\n    \\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}\n\\end{equation*}\\]\n\\[\\begin{equation*}\n    \\sigma_{\\bar{y}} = \\frac{9}{\\sqrt{81}} = 1\n\\end{equation*}\\]\nWhat is important to bear in mind for the construction of confidence intervals, is that we do not just need the right-tail probability, because the area we are trying to cover under the distribution is symmetrical around the mean. This is visualised in Figure 1 where the area is defined between \\(\\pm\\) 1.96 standard deviations around the mean.\n\n\n\n\n\n\nFigure 1: 95 Percent Confidence Interval around the Mean\n\n\n\nAs the orange area needs to be 95%, the remaing areas to the left and right need to be equal to 5% jointly. So each of them is 2.5%. When we look into our Table with right-tail probabilities, we therefore need to look for the z-score that corresponds to 0.025 (or 2.5%). When you look in the Normal Table (see Statistical Tables), then you will find this at z=1.96.\nNow, the question arises, how many standard deviations we need for a 99% confidence interval. The area to the left and right needs to be jointly 1%, or 0.05% each side. We consult the Normal Table again, and try to find the z-score for 0.005. The exact value is not available, only either 0.0051, or 0.0049. 0.0051 would lead to a confidence interval of 98.98%, so not quite large enough. We therefore need to go for 0.0049, and the corresponding z-score of z=2.58.\n\\[\\begin{equation*}\ny = \\bar{y} \\pm 2.58 \\times \\sigma_{\\bar{Y}}\n\\end{equation*}\\]\nPopping the values in we receive\n\\[\\begin{equation*}\n26 \\pm 2.58 \\times 1\n\\end{equation*}\\]\nAs a result, we can say that we are 99% confident that the true average age of students at Warwick lies between 23.42 and 28.58. Note that this confidence interval is wider than the 95% one where the boundaries were defined as 24.04 and 27.96. As we went for higher certainty here, the confidence interval became wider. If we want our interval to contain the true average in 99 out of 100 samples, we need to cast our net wider than if we were content with 95.\n\n\n2. We don’t know \\(\\sigma\\)\nIf we don’t know the population distribution and its standard deviation, we need to use the t-distribution. As you know from the lecture, the t-distribution is a shape shifter. Its width depends on the degrees of freedom: the more degrees of freedom we have, the more narrow, or the closer to the normal distribution it comes. With df=30 the shapes of the t-distribution and the normal distribution are almost identical. You can see this in the following Figure, comparing the yellow t-distribution for \\(df=30\\) and the black (normal) distribution.\n\n\n\n\n\n\nFigure 2: Comparison of t-Distributions\n\n\n\nIn reversed logic, this also means that the t-distribution is rather wide for small sample sizes (and small df). Consequently, a confidence interval of a given level (e.g. 99%) would be much wider for a small sample size than under the normal distribution (or t-distributions with higher sample sizes). Let me illustrate this using the same sample average as for the normal distribution example above, \\(\\bar{y}=26\\). We have a sample standard deviation (\\(s\\)) of 2. Our sample size is very small, we only have \\(n=4\\). Again, we want an interval within which we find with 99% confidence the true average age of students.\nWe start by estimating the standard error:\n\\[\\begin{equation*}\n    se=\\frac{s}{\\sqrt{n}}\n\\end{equation*}\\]\n\\[\\begin{equation*}\nse=\\frac{2}{\\sqrt{4}}=1\n\\end{equation*}\\]\nThis time we have to search in the t-Table, taking into account the degrees of freedom. As \\(n=4\\), this means that \\(df=3\\). Conveniently, the Table lists at the top the desired confidence interval. For \\(df=3\\) and a 99% confidence interval, the corresponding t-value is 5.841. Once again, we calculate:\n\\[\\begin{equation*}\n\\bar{y} \\pm 5.841 \\times se\n\\end{equation*}\\]\nand\n\\[\\begin{equation*}\n26 \\pm 5.841 \\times 1\n\\end{equation*}\\]\nThe resulting boundaries are 20.159 as the lower boundary, and 31.841 as the upper boundary. This is far wider than the 99% confidence interval we obtained for the normal distribution, where the lower and upper boundaries were 23.42 and 28.58, respectively. This is a reflection of the fact that we only had a very small sample (\\(n=4\\)) to base our inference on and therefore have a lot of uncertainty in our inference.\nTo conclude, it is fair to say that the procedure is essentially the same as with the normal procedure, only that have to go the extra step of taking into account the degrees of freedom.\nYou are now ready to do some exercises.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO11Q.html#exercises",
    "href": "08-PO11Q.html#exercises",
    "title": "Week 8",
    "section": "Exercises2",
    "text": "Exercises2\nLet’s get started by exploring the dynamics of a confidence interval more generally. You can answer these questions with the app below.\n\nExploring the role of sample size\n\nSet the confidence level to \\(95\\%\\) and the sample SD to \\(s=0.9\\). Vary \\(n\\) from \\(3\\) to \\(30\\) and record the CI half-width \\(t^* \\cdot \\dfrac{s}{\\sqrt{n}}\\) for \\(n \\in \\{3,5,10,15,20,30\\}\\).\n\nConfidence level and \\(t^*\\)\n\nFix \\(n=10\\) and \\(s=0.9\\). Change the confidence level among \\(80\\%, 90\\%, 95\\%, 99\\%\\) and note the corresponding \\(t^*\\) values from the app.\n\nExplain why \\(t^*\\) (and the CI width \\(2\\,t^*\\,s/\\sqrt{n}\\)) increases with the confidence level.\n\nFor \\(n=10\\) and \\(s=0.9\\), by what factor does the CI width at \\(99\\%\\) exceed the width at \\(90\\%\\)? Calculate \\(\\dfrac{t^*_{0.005,\\,9}}{t^*_{0.05,\\,9}}\\).\n\nTrade-off between variability and sample size\n\nWith level \\(95\\%\\), compare CI half-widths for \\((n,s)=(8,0.6)\\) and \\((n,s)=(20,1.0)\\). Which is wider and why? Use \\(t^* \\cdot s/\\sqrt{n}\\).\n\nSuppose you must keep \\(s=0.9\\) but you want the half-width \\(\\le 0.30\\) at \\(95\\%\\). Use the app to find the smallest \\(n\\) achieving \\(t^* \\cdot \\dfrac{0.9}{\\sqrt{n}} \\le 0.30\\).\n\nHolding \\(n=12\\) fixed, find \\(s\\) such that the full CI width is exactly \\(0.9\\) at \\(90\\%\\). Solve \\(2\\,t^* \\cdot \\dfrac{s}{\\sqrt{12}} = 0.9\\) by adjusting \\(s\\) in the app.\n\n\n\n\n\n\n\nNow, let’s do some calculations by hand:\n\nThe number of diners visiting a restaurant on a Thursday is normally distributed with a mean of 150 and standard deviation of 30. One Thursday only 100 people eat in the restaurant, and the manager says, “next week will be better”.\n\nWhat is the probability she is right?\nThe number of diners on a Friday is also normally distributed with a mean of 200 and a standard deviation of 50. Which two values, symmetrical around the mean contain the number of Friday diners 80% of the time?\n\n\nA researcher is analysing individuals’ relative fear of being a victim of burglary on a 1-100 scale. This variable is normally distributed. A random sample of 9 individuals found a mean score of 47 on the scale with a sample variance of 158.76 for fear of being burgled.\n\nWhat distribution would be used to calculate an 80% confidence interval around this mean?\nConstruct that interval.\nIs this a suitable sample size for seeing whether individuals are more nervous around burglary or murder, which is found to have an 80% confidence interval between 2.76 and 14.65?\n\n\nWe are investigating the height of men in the UK. For this we have obtained a random sample of 100 UK men and found they had a mean height of 180cm with a standard deviation of 10cm.\n\nConstruct a 95% confidence interval for the mean height of UK males.\nSelect all true statements concerning the constructed confidence interval and justify your choice for each statement.\n\nThe probability of the population mean being within the upper and lower bounds is 95%.\n95% of men’s heights fall between the upper and lower bound.\n95% of the cases in the sample fall between the upper and lower bound.\nOn average 95% of confidence intervals constructed would contain the population mean.\nOn average 95% of the means of samples with 100 respondents will fall within the upper and lower bands.\nOn average 95% of the sample means equal the population mean.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO11Q.html#r-exercises",
    "href": "08-PO11Q.html#r-exercises",
    "title": "Week 8",
    "section": "R Exercises",
    "text": "R Exercises\n\nIntroduction\nFor the purpose of this worksheet you will be using a replication data set from Dorff (2011) which provides replication data for the famous study of the determinants of civil war by Fearon & Laitin (2003). This is the abstract, taken from the article:\n\nAn influential conventional wisdom holds that civil wars proliferated rapidly with the end of the Cold War and that the root cause of many or most of these has been ethnic and religious antagonisms. We show that the current prevalence of internal war is mainly the result of a steady accumulation of protracted conflicts since the 1950s and 1960s rather than a sudden change associated with a new, post-Cold War international system. We also find that after controlling for per capita income, more ethnically or religiously diverse countries have been no more likely to experience significant civil violence in this period. We argue for understanding civil war in this period in terms of insurgency or rural guerrilla warfare, a particular form of military practice that can be harnessed to diverse political agendas. The factors that explain which countries have been at risk for civil war are not their ethnic or religious characteristics but rather the conditions that favor insurgency. These include poverty – which marks financially and bureaucratically weak states and also favors rebel recruitment – political instability, rough terrain, and large populations.\n\nThe fearon dataset contains several variables for each country in 1994, whilst the fearonfull data set contains data for all years from 1945 to 1999. Not all of the operations in the exercises were covered by the material in the lectures / seminars on PO11Q. Some of these rely on the reading, and for some you need to google for help on stackoverflow. You can find a full codebook in Table 1.\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\n\n\n\n\nyear\n1945-1999\n\n\nwar\n0 (no); 1 (yes)\n\n\nef\nethnic fractionalisation (%)\n\n\nrelfrac\nreligious fractionalisation (%)\n\n\npop\npopulation (in 1000s)\n\n\nmtnest\nmountainous terrain (%)\n\n\nelevdiff\nelevation difference of the highest and lowest points (metres)\n\n\npolity2\nregime score: -10 (autocracy) to 10 (democracy)\n\n\ngdpen\nGDP per capita (in 1000s, 1985 US Dollar)\n\n\nOil\nless than 1/3 export revenue from oil (0); 1/3 or more export revenue from oil (1)\n\n\nplural\nshare of largest ethnic group (%)\n\n\n\n\n\n\nTable 1: Codebook to Fearon Data Sets\n\n\n\n\n\n \n\n\nExercises\nThese exercises are deliberately slightly more advanced than those of previous weeks to push you a little. For some of these you will have to use google, as we have not covered all the necessary functions on the module, yet.\n\nProduce a new data frame of df1, df2, df3, df4. For each dataset, arrange them according to countries with:\n\nLowest Ethnic Fractionalisation\nHighest Population\nHighest mountainous terrain\nLowest elevation\n\nCreate a data frame which looks upon countries that used to be British colonies. Transform the British colony variable into a binary factor of “No” and “Yes”.\nIf we order all countries by population size, what is the population size at the 40\\(^{\\text{th}}\\) percentile?\nMake a dummy variable which shows whether or not a country is majority Muslim, given that the requirement of a majority Muslim country is more than 50% of its population. Do not forget to order the factors.\n\nUse cut()\nUse ifelse()\n\nThe Polity IV score measures democracy on a scale from -10 to +10 where -10 is equal to perfect autocracy and +10 equal to perfect democracy. Often, the democratisation literature distinguishes between autocracies, anocracies and democracies. We can achieve this differentiation in the Polity IV score as follows:\n\nAutocracies: -10 to -6\nAnocracies: -5 to +5\nDemocracies: +6 to +10\n\nUsing this categorization of polity score; transform the numerical values of polity scores as follows:\n\nInto an ordered factor\nInto a binary dummy variable of Democracy/Non-Democracy where only the level “Democracy” of the previous step remains as “Democracy”.\n\nFind the mean difference between:\n\nthe share of largest ethnic group and second largest ethnic group.\nthe share of largest ethnic group and second largest ethnic group within former British colonies (using previous exercise no. 4).\n\nLet’s do some more recoding:\n\nCreate an age-group dataframe with the following categories:\n\n\nNA\n0-18\n18-35\n35-50\n50-70\n\n\nTransform the values into numeric values, and separate into a lower and upper category\nCreate a column - Find the mid of each levels\nCreate a column - Find the interval of each levels\n\nFor this exercise use the data frame fearonfull which contains data for all years between 1945 and 1999. Population is defined in terms of 1,000s.\n\nCreate a new data which consists of countries with more than 10000000 people. (large population)\nFind the average of population of each large population country from 1945-1999. Hint: group according to country, then find the mean.\n\nSub-setting data, using the fearonfull data frame:\n\nExtract the necessary variables to compare the social fractionalization between countries.\nRetain the last row of the dataset.\nFilter the dataset with only countries in an ongoing war (variable ended).\nFind the country with an ongoing war in 1999.\n\nGenerate a dataframe and find all countries with GDP per capita greater than the world’s mean in 1999 (Function hint: filter)\nGenerate a dataframe that only consist of data from 1998 and 1999. Change the data format into a wide data using spread for the observations of GDP per capita (you need to use fearonfull again).\nGenerate a data frame consisting of list of countries and their populations in 1945 and 1995. Find the mean of population differences between 1945 and 1995. Find the mean of population differences between 1945 and 1995 (you need to use fearonfull again).\nCalculate the average GDP per capita each oil-producing country has had between 1945 and 1999. Which has the highest mean of GDP per capita (you need to use fearonfull again)?\nFor this exercise use the fearonfull data set.\n\nCheck how many missing values are in the dataset as a whole and in the polity2 variable.\nOmit rows with missing values in GDP per capita and population using the filter function.\n\nUse the data in exercise number 14.\n\nCalculate the averages of GDP per capita and population per country between 1945 and 1999. Find the mean GDP per country.\nFilter the top 10 highest GDP countries\n\nFind the difference between each country’s largest ethnic group and second largest ethnic group. Arrange the countries in ascending order based on the difference\nUsing the data in exercise 16, save the dataframes as:\n\nCSV to be read in excel\nDTA file\nR Data",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO11Q.html#solutions",
    "href": "08-PO11Q.html#solutions",
    "title": "Week 8",
    "section": "Solutions",
    "text": "Solutions\nYou can find the Solutions in the Downloads Section. Please note that the conceptual exercises have a document in the “Documents” Section, whereas the solutions to the R Exercises are in an RScript in the section of the same name.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO11Q.html#footnotes",
    "href": "08-PO11Q.html#footnotes",
    "title": "Week 8",
    "section": "",
    "text": "Some of the content of this worksheet is taken from Linke (forthcoming).↩︎\nThese are taken from Linke (forthcoming).↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "09-PO11Q.html",
    "href": "09-PO11Q.html",
    "title": "Week 9",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO11Q.html#measuring-effect-size",
    "href": "09-PO11Q.html#measuring-effect-size",
    "title": "Week 9",
    "section": "Measuring Effect Size",
    "text": "Measuring Effect Size\nTo measure effect size, we will be using Cohen’s \\(d\\). Cohen’s \\(d\\) describes how large an effect is in units of the standard deviation, so it does not depend on the original measurement scale. For a one-sample test2, this means we look at how far the sample mean is from the hypothesized value, relative to the typical variation in the data.\nFor example, suppose the average exam mark in a sample of students is 42, and the pass mark (our null value) is 40. Rather than just saying “the mean was 42 vs. 40”, Cohen’s \\(d\\) asks: how big is that 2-point difference compared to the spread of marks?\nWe start by identifying the observed difference. With a sample mean of 42 (mean indeed!) and a null hypothesis value of 40, the observed difference is\n\\[\n\\text{Difference} = 42 - 40 = 2\n\\]\nThis raw difference might seem informative at first glance, but it will be hard to compare this across different contexts. Why? Suppose we look at exam marks in two different modules. In course A students’ marks are closely bunched together, most within about 5 points of the average. In this setting, a 2-point increase is fairly big compared to the usual spread. In module B, meanwhile, students’ marks vary a lot, often 20 points above or below the average. In this setting, the same 2-point increase is tiny compared to the usual spread.\nThis is why we cannot stop at raw differences – we need to find a way to standardise the difference so that we can compare across different contexts. And this is precisely what Cohen’s \\(d\\) does. It is defined as\n\\[\nd = \\frac{\\bar x - \\mu_0}{\\text{s}}\n\\]\nwhere \\(\\mu_0\\) is the null hypothesis value. This formula standardises the raw distance by dividing it by the standard deviation (s) of marks.\nTo continue with our example, if the typical standard deviation of exam marks is about 10 points, then:\n\\[\nd = \\frac{42 - 40}{10} = \\frac{2}{10} = 0.20\n\\]\nSo our observed effect is \\(d = 0.20\\). How do we interpret this result? Cohen (2013) suggested some rough rules of thumb for interpreting \\(d\\):\n\nInterpretation\n\n\\(d \\approx 0.2 \\Rightarrow\\) small effect (a subtle shift, often hard to detect)\n\n\\(d \\approx 0.5 \\Rightarrow\\) medium effect (a clear, noticeable shift)\n\\(d \\approx 0.8 \\Rightarrow\\) large effect (a big, obvious shift)\n\nNote that these are rough guidelines, only. The onus is on you as a researcher to judge what is required/acceptable in the context of your project.\n\nA value of \\(d = 0.20\\) in our example means the average mark is 0.2 standard deviations above the null value. That is considered a small effect: the group did a little better than the pass mark, but the difference is modest relative to the variability in marks.\nIn summary, Cohen’s \\(d\\) encourages us to think not just about whether an effect exists, but whether it is meaningfully large. We are now going to use it to determine the statistical power of a one-sample significance test. You will see that smaller \\(d\\) values need larger sample sizes to be detected reliably.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO11Q.html#what-is-statistical-power",
    "href": "09-PO11Q.html#what-is-statistical-power",
    "title": "Week 9",
    "section": "What is statistical power?",
    "text": "What is statistical power?\nStatistical power is the probability that a study will detect an effect if the effect really exists. Formally, we can write this as:\n\\[\n\\text{Power} = P(\\text{reject } H_0 \\mid \\text{effect exists})\n\\]\n\nHigh power means we are likely to detect real effects.\n\nLow power means we might miss them, even if they exist.\n\nA common benchmark is 80% power (see Lakens (2013) and Button et al. (2013)). This means: if the true effect size (\\(d_{\\text{true}}\\)) is at least as large as the effect we are testing for, then with the same sample size and test procedure we would, in the long run, detect it in 80% of studies.\nNote that this true effect size only exists in the population — it reflects the actual standardized difference. This is different from the observed effect size (\\(d_{\\text{obs}}\\)), which will vary from sample to sample. In practice, we often use an assumed effect size (based on prior studies, conventions, or theory) when planning a study, and say: “if \\(d_{\\text{true}}\\) is at least this big, then with \\(n\\) participants we’ll have 80% power.”\nPower therefore depends on two things: the size of the true effect and the size of the sample. Let’s return to our earlier example of exam marks to illustrate this. Here, we were testing whether the average exam mark is higher than the pass mark of 40. Imagine that, in the population, the true average is 42 and the population standard deviation is about 10. This means the true effect size is\n\\[\nd_{\\text{true}} = \\frac{42 - 40}{10} = 0.2.\n\\]\nIf we take a sample of 25 students, our study might have only around 40% power. In other words: even though the true mean really is 2 points above 40 (so \\(d_{\\text{true}} = 0.2\\)), our test would only reject \\(H_0\\) in about 4 out of 10 repeated studies.\nIf instead we sampled 100 students, the power would increase to about 80%. That means in the long run we would correctly detect this true 2-point effect about 8 times out of 10.\nIn any particular sample, we could calculate an observed effect size \\(d_{\\text{obs}}\\) (using the sample mean and standard deviation). Sometimes \\(d_{\\text{obs}}\\) will be close to 0.2, sometimes higher, sometimes lower — but the power calculation is always based on the underlying \\(d_{\\text{true}}\\).\n\nSo, how would you set a value for \\(d\\) if you don’t know the population?\nSince the true effect size \\(d_{\\text{true}}\\) is unknown, we plan a study by working with an assumed effect size. This assumed value can come from:\n\nPrevious studies or meta-analyses: using their reported \\(d_{\\text{obs}}\\) as a rough guide.\n\nTheoretical expectations: deciding what size of effect would be meaningful in context (sometimes called the “smallest effect size of interest”).\n\nConventions: Cohen suggested rough benchmarks (\\(d = 0.2\\) small, \\(0.5\\) medium, \\(0.8\\) large), though these should be treated with caution.\n\nOnce we pick an assumed effect size, we can calculate how large a sample \\(n\\) is needed to achieve, say, 80% power. If the true effect size \\(d_{\\text{true}}\\) really is at least that big, then our study will have the planned power.\n\nBut how do we get from \\(d\\) to power? The link is through the \\(t\\)-statistic. When you read the formula and description for Cohen’s \\(d\\) earlier, you might have had a déjà vu, because the \\(t\\)-score is defined as\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{\\text{se}}, \\qquad \\text{where se} = \\frac{s}{\\sqrt{n}}.\n\\]\nThis means that \\(t\\) and \\(d\\) are very closely related, namely through this little exercise in algebra:\n\\[\\begin{align}\nt &\\;=\\; \\frac{\\bar{x}-\\mu_0}{\\,s/\\sqrt{n}\\,} = \\frac{(\\bar{x}-\\mu_0)\\cdot(1/s)}{(s/\\sqrt{n})\\cdot(1/s)} \\;= \\\\ \\\\\n  &\\;=\\; \\frac{(\\bar{x}-\\mu_0)/s}{(s/\\sqrt{n})/s} \\;=\\; \\frac{(\\bar{x}-\\mu_0)/s}{\\,1/\\sqrt{n}\\,} \\;= \\\\ \\\\\n  &\\;=\\; \\left( \\frac{\\bar{x}-\\mu_0}{s} \\right) \\cdot \\frac{\\sqrt{n}}{1}  \\;=\\; \\left( \\frac{\\bar{x}-\\mu_0}{s} \\right) \\cdot \\sqrt{n} \\;= \\\\ \\\\\n  &\\;=\\; d\\cdot \\sqrt{n}.\n\\end{align}\\]\n\n\\(d\\) tells us the size of the effect relative to the spread.\n\n\\(\\sqrt{n}\\) tells us how sample size sharpens our estimate: larger \\(n\\) shrinks sampling noise.\n\nSo for a fixed \\(d\\), increasing \\(n\\) makes \\(t\\) larger and pushes us past the critical threshold for significance more often — which is exactly what raises power. The key is that statistical power is defined as the probability of correctly rejecting the null when the effect size is truly \\(d\\). Equivalently, it is the probability that the test statistic \\(t\\), which depends on both \\(d\\) and \\(n\\), will exceed the critical cut-off. Thus, as \\(n\\) increases, the distribution of \\(t\\) shifts further into the rejection region, and the probability of crossing the cut-off – the power – goes up.\nThe calculation of power itself is slightly beyond the scope of an introductory level module, because we need a different distribution, a noncentral \\(t\\) distribution to be precise, for this purpose. Instead, I have designed an app which calculates this automatically for you (see below). What might be interesting to know in the context of the lecture, however, is that power is equal to \\(1-\\beta\\) where \\(\\beta\\) is the Type II Error. Let’s conclude this section with the following statements:\n\n\nFor a small effect (\\(d = 0.20\\)), increasing \\(n\\) pushes \\(t\\) upward by \\(\\sqrt{n}\\).\n\nWith small \\(n\\), sampling noise keeps \\(t\\) below the cut-off most of the time (low power).\n\nWith larger \\(n\\), the standard error shrinks, \\(t\\) grows, and power increases.\n\nIn terms of the exam example, you could use this information for planning a survey: if you care about detecting a small but meaningful 2-point difference, you generally need larger samples than 25 – often well above 100 – to achieve comfortable power (for example, around 80%).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO11Q.html#an-application",
    "href": "09-PO11Q.html#an-application",
    "title": "Week 9",
    "section": "An Application",
    "text": "An Application\nTo illustrate how this works in practice, I have written an application that lets you evaluate a completed one-sample t-test and plan a replication. You enter the results you observed in that test, and the app shows the statistical power at your observed \\(n\\) (two-sided, at your chosen \\(\\alpha\\)). As a special feature, you can switch to the “smallest effect size of interest” (SESOI), which lets you plan the \\(n\\) required for a meaningful effect.\n\n  Open Application      \n\n\nHere is how to use it:\n\nEnter the observed t-statistic and the observed sample size \\(n\\).\nChoose \\(\\alpha\\) (significance level).\nInterpret the statistical power of your test.\nOptional: turn on SESOI and pick a Cohen’s \\(d\\) (Small \\(\\approx 0.2\\), Medium \\(\\approx 0.5\\), Large \\(\\approx 0.8\\)).\n\nSESOI tells the app that only effects at least this big are meaningful.\n\nIf SESOI is off, the app uses the observed effect \\(\\hat d = t/\\sqrt{n}\\).\n\nIf SESOI is on, the app fixes the curve at your chosen \\(d\\) and, for convenience, shows the required \\(n\\) for 80% power as an orange dotted line.\n\n\nIn order to choose \\(d\\) appropriately, pick a minimum meaningful change in real units and divide by a typical standard deviation. For example, if you think that a difference of 2 exam points from an assumed null value (such as the fail mark of 40) matters and the SD is about 10, then \\(d \\approx 2/10 = 0.2\\) (small).\n\n\n\nHow the “Power vs n” curve is calculated\n\nWhen you choose a fixed standardized effect size \\(d\\) (either your observed \\(\\hat d = \\tfrac{t}{\\sqrt{n_{\\text{obs}}}}\\) or a SESOI), the curve shows the power of a two-sided one-sample \\(t\\)-test across many candidate sample sizes \\(n\\).\nFor each \\(n\\):\n\nDegrees of freedom: \\(\\mathrm{df} = n - 1\\)\n\nNoncentrality parameter: \\(\\delta = d\\sqrt{n}\\)\n\nCritical cut-off (two-sided, level \\(\\alpha\\)): \\(t_{\\alpha/2,\\,\\mathrm{df}}\\)\n\nUnder a true effect \\(d\\), the test statistic follows a noncentral \\(t\\) distribution:\n\\[\nT \\sim t_{\\mathrm{df}}(\\text{ncp} = \\delta)\n\\]\nThe power is the probability that \\(|T|\\) exceeds the critical cut-off:\n\\[\n\\mathrm{power}(n)\n= \\Pr\\!\\left(|T|\\ge t_{\\alpha/2,\\ \\mathrm{df}} \\,\\middle|\\, \\delta\\right)\n= \\big[1 - F_{\\text{nct}}(t_{\\alpha/2,\\ \\mathrm{df}};\\ \\mathrm{df},\\ \\delta)\\big]\n+ F_{\\text{nct}}(-t_{\\alpha/2,\\ \\mathrm{df}};\\ \\mathrm{df},\\ \\delta)\n\\]\nwhere \\(F_{\\text{nct}}\\) is the CDF of the noncentral \\(t\\) distribution.\nWhat the curve means\n\nSESOI off: The curve treats the observed \\(\\hat d = t_{\\text{obs}}/\\sqrt{n_{\\text{obs}}}\\) as the “true” effect and shows how power would change if you repeated the study with different \\(n\\).\n\nSESOI on: The curve instead uses your chosen meaningful effect size \\(d\\). In this mode, the app also shows the required \\(n\\) to reach 80% power with an orange marker.\n\nReproducing the curve in R\n\nlibrary(tidyverse)\n\n# Power of a two-sided one-sample t-test via the noncentral t distribution\npower_one_sample &lt;- function(d, n, alpha = 0.05) {\n  df    &lt;- n - 1\n  tcrit &lt;- qt(1 - alpha/2, df = df)\n  delta &lt;- d * sqrt(n)\n  p_upper &lt;- 1 - pt(tcrit, df = df, ncp = delta)\n  p_lower &lt;-     pt(-tcrit, df = df, ncp = delta)\n  p_upper + p_lower\n}\n\n# Example settings\nalpha &lt;- 0.05\nd_use &lt;- 0.50          # e.g., \"medium\" SESOI; or d_use &lt;- tobs/sqrt(nobs)\nn_seq &lt;- 5:200\n\n# Compute the curve\npower_vals &lt;- sapply(n_seq, power_one_sample, d = d_use, alpha = alpha)\ndf_curve   &lt;- data.frame(n = n_seq, power = power_vals)\n\n# Optional: check against power.t.test at a single n\nn_check &lt;- 30\np1 &lt;- power_one_sample(d = d_use, n = n_check, alpha = alpha)\np2 &lt;- stats::power.t.test(\n  n = n_check, delta = d_use, sd = 1,\n  sig.level = alpha, type = \"one.sample\", alternative = \"two.sided\"\n)$power\nsprintf(\"Check at n=%d: custom=%.4f, power.t.test=%.4f\", n_check, p1, p2)\n\n[1] \"Check at n=30: custom=0.7540, power.t.test=0.7540\"\n\n# Plot the curve\nggplot(df_curve, aes(n, power)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = 0.80, linetype = 2) +\n  coord_cartesian(ylim = c(0, 1)) +\n  labs(\n    title = sprintf(\"Power vs n (one-sample t-test; d = %.2f, alpha = %.2f)\", d_use, alpha),\n    x = \"Sample size n\",\n    y = \"Power\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nWhy sd = 1 in the check? Because Cohen’s \\(d\\) is already standardized (\\(d = \\Delta/\\sigma\\)). The function power.t.test expects the unstandardized difference \\(\\Delta\\) and the population standard deviation \\(\\sigma\\). Setting sd = 1 and delta = d makes the inputs consistent with a standardized effect size.\n\n\n\n\nHow to read the Power vs \\(n\\) plot\n\nAxes and lines\n\nX-axis: sample size \\(n\\) (one-sample test).\nY-axis: power (0 to 1).\nSolid curve: analytical power for the current effect and \\(\\alpha\\).\n\nWith SESOI off: uses \\(\\hat d = t/\\sqrt{n_{\\text{obs}}}\\).\nWith SESOI on: uses your chosen \\(d\\) (SESOI).\n\nDashed horizontal line at 0.80: a common 80% power target.\nDotted vertical line (grey): your observed \\(n\\); the dot shows power at that \\(n\\).\nDotted vertical line (orange): shown only when SESOI is on; marks the smallest \\(n\\) that reaches 0.80 power (labelled as \\(n \\approx \\dots\\) @ 80% power).\n\nFinding the sample size you need\n\nWith SESOI on, read the orange line and label to get the sample size to plan for 80% power at your chosen \\(\\alpha\\).\nIf the curve never hits 0.80 within the plot range, you would need a larger \\(n\\), a larger meaningful effect (bigger \\(|d|\\)), a one-sided test (only if justified), or a larger \\(\\alpha\\) (with caution).\n\nHow the controls shift the curve\n\nLarger \\(|d|\\) moves the curve up and left (fewer \\(n\\) needed).\nIncreasing \\(\\alpha\\) (for example, 0.10 vs 0.05) moves the curve up; decreasing \\(\\alpha\\) moves it down.\nChanging the observed \\(t\\) or \\(n\\) with SESOI off updates \\(\\hat d\\) and thus the curve. With SESOI on, the curve stays fixed by your chosen \\(d\\); changing \\(n\\) only moves the dot.\n\nShape and diminishing returns\n\nThe curve is steepest at small \\(n\\) and flattens as power approaches 1.\nBecause \\(t = d\\sqrt{n}\\), halving \\(|d|\\) typically requires about four times the \\(n\\) to maintain similar power.\n\nInterpreting the dot and the lines\n\nThe dot is your current power at the observed \\(n\\).\nIf the dot is below 0.80, you likely need a bigger \\(n\\) (or a larger \\(d\\) of interest, higher \\(\\alpha\\), or a justified one-sided test).\n\nRemember\n\nPower is the probability to reject the null-hypothesis given that an effect exists, or formally \\(P(\\text{reject } H_0 \\mid \\text{effect exists})\\). It is not a \\(p\\)-value and not the probability that \\(H_0\\) is true.\nYou can pair power planning with a SESOI: decide what \\(d\\) would be meaningful in your context and size your study to detect it.\n\n\n\nWhat this means in practice:\n\nThat a result is statistically significant does not mean that it is practically important. With very large \\(n\\), even tiny effects can be significant.\n\nEqually, insignificant results do no mean “no true effect”. With small \\(n\\), a real effect can be missed because power is low.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO11Q.html#conceptual---working-with-the-app",
    "href": "09-PO11Q.html#conceptual---working-with-the-app",
    "title": "Week 9",
    "section": "Conceptual - Working with the App",
    "text": "Conceptual - Working with the App\n\nSame effect size, different n (mapping \\(t\\), \\(n\\), and \\(\\hat d\\))\n\nSet observed \\(t=1.6\\) and \\(n=16\\). Note \\(\\hat d = t/\\sqrt{n}\\) and the power at the observed \\(n\\).\n\nNow keep \\(\\hat d\\) the same but increase \\(n\\): set \\(n=64\\) and \\(t=3.2\\) (since \\(t = d \\times \\sqrt{n}\\)). Confirm that \\(\\hat d\\) is unchanged while power increases.\n\nExplain why power rises with \\(n\\) even when the underlying effect size \\(\\hat d\\) stays the same.\n\nPlanning with a SESOI and \\(\\alpha\\) sensitivity\n\nEnter \\(t=2.0\\) and \\(n=30\\) (\\(\\alpha=0.05\\)). Record \\(\\hat d\\) and the power at the observed \\(n\\).\n\nTurn on “Use smallest effect size of interest” and choose SESOI \\(d=0.5\\). Read the orange marker: the required \\(n\\) for \\(80\\%\\) power at \\(\\alpha=0.05\\).\n\nChange \\(\\alpha\\) to \\(0.10\\) and then to \\(0.01\\). How does the required \\(n\\) for \\(80\\%\\) power shift as \\(\\alpha\\) changes? Explain why a stricter \\(\\alpha\\) requires a larger \\(n\\) for the same power.\n\nWas the study well powered? Post-hoc check and replication planning\n\nSuppose your study reported \\(t=2.1\\) with \\(n=25\\) at \\(\\alpha=0.05\\). Enter these and record \\(\\hat d\\) and the power at the observed \\(n\\). Is the power comfortably high?\n\nIf you wanted \\(80\\%\\) power to detect an effect around this magnitude in a replication, turn on SESOI and choose the closest option to your \\(\\hat d\\) (for example, Small \\(d=0.2\\) if \\(\\hat d\\) is small, Medium \\(d=0.5\\) if \\(\\hat d \\approx 0.5\\)). Note the orange “\\(n\\) for \\(80\\%\\) power” marker.\n\nExplain why a significant result with low power can be fragile, and how choosing a SESOI (a target \\(d\\)) helps you plan a more reliable replication sample size.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO11Q.html#r-exercises",
    "href": "09-PO11Q.html#r-exercises",
    "title": "Week 9",
    "section": "R Exercises",
    "text": "R Exercises\nThese exercises will use the ks2.csv dataset. This data comprises fictitious3 average grades of Key Stage 2 (KS2) students in the UK, with 1,980 KS2 students’ test scores being included for reading (reading), mathematics (maths), and grammar, punctuation, and spelling (gps), as well as the mean of these three test scores (avg\\_all). The test scores have been standardised, with 80 representing the lowest possible mark, 120 representing the highest possible mark, 100 representing the minimum passing mark, and -1 representing an ungraded test.\n\nLoad the ks2.csv dataset into R. Remove any observations that have any ungraded test scores.\nCalculate the means and standard deviations of each of the three subjects. Write a brief description of the insights that can be drawn from these values.\nConduct a t-test to see if the means of each of the three subjects’ marks are statistically different from 100 at the 95% confidence level. Identify three ways that suggest statistically significant or insignificant differences.\nConduct a t-test to see if the mean of the average score of the three tests is statistically less than 105 at the 99% confidence level. Interpret the results.\nThe following questions will look at students’ English abilities generally.\n\nCreate a new variable called english, which consists of the average of the reading and grammar, punctuation, and spelling variables.\nConduct a t-test to see if the mean of the average score of the new English variable is statistically less than 105 at the 99.9% confidence level. Interpret the results.\nConduct a t-test to see if the mean of the average score of the new English variable is statistically different from 105 at the 99.9% confidence level. Interpret the results.\nIs the any difference in the interpretation between the two above tests? Are there any differences in the results? Why?\n\nYou are tasked with investing the performance of students who passed in mathematics those who did not. For the following tests, use a 95% confidence level.\n\nCreate a binary variable that has two categories: those who passed mathematics (100 \\(\\leq\\) mark) and those who failed mathematics (mark \\(&lt;\\) 100).\nConduct a proportion test to see if the proportion of students that fail mathematics is 10% or greater. Interpret the results.\nConduct a t-test on the group who fail mathematics to see if they, on average, have marks for English statistically less than 100. Interpret the results.\nConduct a t-test on the group who pass mathematics to see if they, on average, have marks for grammar, spelling, and punctuation statistically greater than 105. Interpret the results.\n\nNow it is worth investigating how students who fail at least one subject perform.\n\nCreate a binary variable that has two categories: those who passed all three subjects (all marks greater than or equal to 100) and those who failed at least one subject (one or more marks less than 100).\nIt can be hypothesised that the group of students who failed will have a mean of all of the test marks significantly below the pass mark of 100. Test this and interpret the findings with respect to the statistical and practical significance of the test.\n\nImagine you are part of a team work working within the Department for Education, tasked with investigating this sample to produce recommendations for policymakers.\n\nNormalise the variable that contains the average of all three marks by setting the lowest mark (80) to 0, the highest mark (120) to 100, and the minimum pass mark (100) to 50. Justify why it might be useful to normalise these marks to this scale for non-specialist policymakers.\nConstruct a categorical variable that consists of five categories: 0-49.99 (Fail), 50-59.99 (Pass), 60-60.99 (Merit), 70-79.99 (Distinction), and 80+ (Distinction+).\nAnswer the following questions but write your answers as if intended for a non-specialist policy making with no knowledge of statistics:\n\nAre the averages of the Pass, Merit, and Distinction groups different from their middle marks (55, 65, and 75, respectively)? If so, which direction?\nIs the average mark of the Distinction+ group lower than the maximum mark of the group?\nIs the mean mark of the Fail group higher than the median mark of the group? Which skew does this indicate in the distribution?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO11Q.html#solutions",
    "href": "09-PO11Q.html#solutions",
    "title": "Week 9",
    "section": "Solutions",
    "text": "Solutions\nYou can find the Solutions in the Downloads Section.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO11Q.html#footnotes",
    "href": "09-PO11Q.html#footnotes",
    "title": "Week 9",
    "section": "",
    "text": "All exercises are a reproduction from Linke (forthcoming).↩︎\nWe can extend this idea to comparing two groups (e.g., income of men and women) by looking at the difference in their means relative to the variation within groups.↩︎\nThe means are based on KS2 scaled score averages which have been averaged over 2016-2019.↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "10-PO11Q.html",
    "href": "10-PO11Q.html",
    "title": "Week 10",
    "section": "",
    "text": "Road Map for Today\n\n\n\n\n\n\n\nSelf-Assessment Questions\n\n\n\n\nExam Preparation\n\n\n\n\nBreak\n\n\n\n\nExam Preparation\n\n\n\n\n\n\n\n\n\nSelf-Assessment Questions\n\nHow do causality in every day life and causality in statistics differ?\nExplain the role of symmetry and asymmetry in causality.\nGive an example for a counterfactual.\nWhy is the historical context of the analysis important for establishing causality?\nHow do the attributes of causality relate to the topics on PO11Q?\n\n\nPlease stop here and don’t go beyond this point until we have compared notes on your answers.\n\n\n\nExam Preparation\nWe will go through the solutions to the mock exam today. Please prepare this exam and note questions about the module content which we will discuss.\n\n\nHomework\n\nPrepare yourself for the exam in January\n\n\n\nGlossary\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDescription\n\n\n\n\nasymmetry\nThe notion that while X causes Y, Y does not cause X. It is established with temporal priority, manipulated events, and/or the independence of causes (see Brady (2011))\n\n\ncausal\nIn order to establish a causal relationship, the following criteria must be met concurrently:  \n\n\ncausal loop\nY causes itself (e.g., health; happiness, wealth)\n\n\ninteraction effect\nAn interaction effect occurs when the effect of one independent variable, \\(x_1\\), on a dependent variable, \\(y\\), depends on the level of another independent variable, \\(x_2\\). In other words, the impact of \\(x_1\\) on \\(y\\) is not constant but varies depending on the value of \\(x_2\\), indicating that the two variables jointly influence the outcome in a non-additive way\n\n\nintervening variable\nAn intervening variable is a variable that explains the relationship between two other variables, such that \\(x_1\\) affects \\(x_2\\), and in turn, \\(x_2\\) affects \\(y\\). In this case, the effect of \\(x_1\\) on \\(y\\) is indirect, operating through the intervening variable \\(x_2\\). This is also referred to as indirect causality\n\n\nmulticausality\nMulticausality refers to a situation in which an outcome variable \\(y\\) is influenced by multiple independent variables, such that \\(x_1\\), \\(x_2\\), \\(x_3\\), all contribute to explaining variations in \\(y\\). In this context, no single variable fully accounts for changes in \\(y\\); instead, the outcome is the result of the combined effects of several causes. We distinguish a speacial case here in which the independent variables are interdependent\n\n\nreverse causality\nCausality runs from x to y. If y could also cause x, then we deal with reverse causality\n\n\nspurious\nA relationship between two variables, \\(x_1\\) and \\(y\\), is said to be spurious when it appears to exist in a simple analysis but disappears or weakens substantially once a third variable, \\(x_2\\), is introduced into the analysis. This suggests that the observed association between \\(x_1\\) and \\(y\\) was not causal, but rather the result of a confounding influence from \\(x_2\\)\n\n\nsuppressor\nA suppressor variable is a variable, \\(x_2\\), that increases the predictive validity of another variable, \\(x_1\\), on an outcome \\(y\\) by accounting for irrelevant or misleading variance in \\(x_1\\). In other words, the relationship between \\(x_1\\) and \\(y\\) becomes stronger or more apparent when \\(x_2\\) is included in the analysis, even though \\(x_2\\) may have little or no direct relationship with \\(y\\)\n\n\nsymmetry\nIn the context of causation, symmetry is understood as the law-like regularity of events. There needs to be a recipe (causal mechanism) which regularly produces effects from causes (see Brady (2011))\n\n\n\n\n\n\nTable 1: Glossary Week 10\n\n\n\n\n\n \n\n\nFlashcards\n\n\n\nAll R Functions on the Module\n\n\n \n\n\n \n\nThe data are available as a .csv file.\n\n\n\n\n\n\nBrady, H. E. (2011). Causation and Explanation in Social Science. In R. Goodin (Ed.), The Oxford Handbook of Political Science. Oxford University Press. https://doi.org/10.1093/oxfordhb/9780199604456.013.0049",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "11-Downloads.html",
    "href": "11-Downloads.html",
    "title": "Downloads",
    "section": "",
    "text": "Introduction\nYes, all solutions and all RScripts are available to you without silly time restrictions. The reason is that I have come to realise that I am operating a module in a university, and not in a kindergarten. Of course, you can download and look at the solutions before you have given the exercises a go yourself first. By all means, cheat. But I can’t promise you that you will learn very much. It’s your choice.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Downloads</span>"
    ]
  },
  {
    "objectID": "11-Downloads.html#documents",
    "href": "11-Downloads.html#documents",
    "title": "Downloads",
    "section": "Documents",
    "text": "Documents\n\nPO11Q Bibliography\nStatistical Tables\nFormula Collection\nQS_POQ_Essay and Dissertation Guidelines\n\n \n\nWeek 4 Worksheet Solutions\nWeek 7 Worksheet Solutions\nWeek 8 Worksheet Solutions\nWeek 9 Worksheet Solutions",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Downloads</span>"
    ]
  },
  {
    "objectID": "11-Downloads.html#data-sets",
    "href": "11-Downloads.html#data-sets",
    "title": "Downloads",
    "section": "Data Sets",
    "text": "Data Sets\n\nEU.xlsx\nfearon.dta\nfearonfull.dta\nks2.csv",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Downloads</span>"
    ]
  },
  {
    "objectID": "11-Downloads.html#r-scripts",
    "href": "11-Downloads.html#r-scripts",
    "title": "Downloads",
    "section": "R Scripts",
    "text": "R Scripts\n\nWeek 7 Script and Solutions\nWeek 8 Solutions\nWeek 9 Solutions",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Downloads</span>"
    ]
  },
  {
    "objectID": "12-Glossary.html",
    "href": "12-Glossary.html",
    "title": "Full Glossary",
    "section": "",
    "text": "Full Glossary\n\n\n\nUnless otherwise noted, the definitions are taken from Linke (forthcoming).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDescription\n\n\n\n\nanalysis\nA detailed evaluation of data to discover their structure and relevant information to answer a research question\n\n\nasymmetry\nThe notion that while X causes Y, Y does not cause X. It is established with temporal priority, manipulated events, and/or the independence of causes (see Brady (2011))\n\n\nattribute\nA component or characteristic of a concept\n\n\nbackground concept\nThe broad constellation of meanings and understandings associated with the concept (Adcock & Collier, 2001, p. 531)\n\n\ncategorical\nDescribing the qualitative categories of a characteristic, for example different religions\n\n\ncausal\nIn order to establish a causal relationship, the following criteria must be met concurrently:  \n\n\ncausal loop\nY causes itself (e.g., health; happiness, wealth)\n\n\ncentral limit theorem\nIn random sampling with a large sample size – where n=30 is usually sufficient – the sampling distribution of the sample mean \\(\\bar{y}\\) will be approximately normally distributed, irrespective of the shape of the population distribution\n\n\nconcept\nAbstract ideas which form the building blocks of theories (Clark et al., 2021, p. 150)\n\n\nconceptualization\nFormulating a systematized concept through reasoning about the background concept, in light of the goals of research (Adcock & Collier, 2001, p. 531)\n\n\nconfidence interval\nA confidence interval is an estimated range, based on a sample, that is likely to contain the true population parameter (such as the mean). If the sampling process were repeated many times, approximately \\((1 - \\alpha) \\cdot 100\\) % of the resulting intervals would contain the true parameter. The value of \\(\\alpha\\) determines the confidence level – for example, \\(\\alpha = 0.05\\) corresponds to a 95% confidence level, while \\(\\alpha = 0.01\\) corresponds to 99%\n\n\nconfidence level\nThe confidence level is the proportion of confidence intervals, constructed from repeated random samples of the same population using the same method, that are expected to contain the true population parameter. It is denoted by \\(1 - \\alpha\\), where \\(\\alpha\\) is the significance level. For example, a 95% confidence level implies that 95% of such intervals would contain the true parameter in the long run.\n\n\nconflation\nA variable does not belong to the attribute in question, but to a different one (Munck & Verkuilen, 2002, pp. 13–14)\n\n\nconstant\nA variable which does not vary\n\n\ncontinuous\nCan assume any value within defined measurement boundaries\n\n\ncritical value\nThe critical value is a threshold that determines the boundary for rejecting the null hypothesis (H\\(_0\\)) in a hypothesis test. It is a point on the probability distribution of the test statistic beyond which the null hypothesis is rejected. The critical value is chosen based on the significance level (\\(\\alpha\\)) of the test, which represents the probability of making a Type I error (i.e., rejecting a true null hypothesis).\n\n\ncross-sectional data\nLook at different units (or cross-sections) \\(i\\) at a single point in time\n\n\ndata\nDerives from the Latin which means . For our purposes it is a collection of numbers (or quantities) for the purpose of analysis\n\n\ndata set\nA collection of numerical values for individual observations, separated into distinctive variables\n\n\ndegrees of freedom\nDegrees of freedom express constraints on our estimation process by specifying how many values in the calculation are free to vary\n\n\ndemocracy\nA system in which the population chooses and holds accountable elected representatives through fair, free, and contested, multi-party elections. The human rights, civil rights, and civil liberties of individuals are protected by law\n\n\ndependent variable\nIs dependent through some statistical or stochastic process on the value of an independent variable\n\n\ndescriptive statistics\nSummarise information about the centre and variability of a variable\n\n\ndeviation\nThe deviation \\(d\\) of an observation \\(y_{i}\\) from the sample mean \\(\\bar{y}\\) is the difference between them: \\(d=y_{i}-\\bar{y}\\)\n\n\ndichotomous\nCan only assume two mutually exclusive, but internally homogeneous qualitative categories\n\n\ndiscrete\nThe result of a counting process\n\n\ndistribution\nRefers to the display of the values a variable can assume, together with their respective absolute or relative frequency\n\n\neffect size\nThe magnitude of the difference or relationship. Larger effects are easier to spot and usually more practically important\n\n\ngeneralisability\nThe ability to apply the findings made on the basis of a representative sample to the population\n\n\nhistogram\nDisplays through rectangles the frequency with which the values of a continuous variable occur in specific ranges\n\n\nhypothesis\nIn statistics, a hypothesis is a formal statement about a population parameter or relationship between variables. Hypotheses guide statistical tests to determine whether data support or refute them. The hypothesis suggesting an effect or difference is called the alternative hypothesis. The alternative hypothesis is always paired with a null-hypothesis, suggesting no effect or difference.\n\n\nindependent variable\nInfluences or helps us predict the level of a dependent variable. It is often treated as fixed, or “given” in statistical analysis, and is sometimes also called “explanatory variable”\n\n\ninteraction effect\nAn interaction effect occurs when the effect of one independent variable, \\(x_1\\), on a dependent variable, \\(y\\), depends on the level of another independent variable, \\(x_2\\). In other words, the impact of \\(x_1\\) on \\(y\\) is not constant but varies depending on the value of \\(x_2\\), indicating that the two variables jointly influence the outcome in a non-additive way\n\n\ninterpretation\nThe explanation of results to answer the research question\n\n\ninterquartile range\nThe difference between the 3\\(^{\\text{rd}}\\) and the 1\\(^{\\text{st}}\\) quartiles\n\n\nintervening variable\nAn intervening variable is a variable that explains the relationship between two other variables, such that \\(x_1\\) affects \\(x_2\\), and in turn, \\(x_2\\) affects \\(y\\). In this case, the effect of \\(x_1\\) on \\(y\\) is indirect, operating through the intervening variable \\(x_2\\). This is also referred to as indirect causality\n\n\nliterature review\nAn analytical summary of the literature relating to a particular topic with the objective of identifying a gap and thus motivating a research question\n\n\nmean\nIs equal to the sum of the observations divided by the number of observations\n\n\nmeasurement\nRefers to the selection of a measure or variable\n\n\nmedian\nSeparates the lower half from the upper half of observations\n\n\nmethod\nA tool for systematic investigation\n\n\nmode\nIs the most frequently occurring value\n\n\nmulticausality\nMulticausality refers to a situation in which an outcome variable \\(y\\) is influenced by multiple independent variables, such that \\(x_1\\), \\(x_2\\), \\(x_3\\), all contribute to explaining variations in \\(y\\). In this context, no single variable fully accounts for changes in \\(y\\); instead, the outcome is the result of the combined effects of several causes. We distinguish a speacial case here in which the independent variables are interdependent\n\n\nnon-probability sampling\nIn non-probability sampling not every unit has the same probability of being sampled\n\n\nnormal distribution\nThe normal distribution is a bell-shaped probability distribution that is symmetrical around the mean. Approximately 68% of values fall within 1 standard deviation of the mean, 96% within 2 standard deviations, and 99.7% within 3 standard deviations. This is known as the empirical rule.\n\n\noutlier\nDefined as a value larger than the third quartile plus 1.5 times the interquartile range, or the first quartile minus 1.5 times the interquartile range\n\n\np-value\nThe p-value indicates the probability of obtaining a result equal to, or even more extreme than the observed value, assuming the null hypothesis is true. Common thresholds for significance are 0.05, 0.01, and 0.001. A smaller p-value suggests stronger evidence against the null hypothesis. The p-value is denoted as \\(p\\).\n\n\nparameter\nA parameter is the value a statistic would assume in the long run. It is also called the Expected Value\n\n\npercentile\nIn ordered data, the percentile refers to the value of a variable below which a certain proportion of observations falls\n\n\npopulation\nCollection of all cases which possess certain pre-defined characteristics\n\n\npopulation distribution\nThe probability distribution of the population\n\n\npower\nAlso known as “sensitivity”. It is the probability that a test will detect a true effect (reject \\(H_0\\) when it is false). Power grows with larger effects and larger samples, and shrinks when you demand stronger evidence (smaller \\(\\alpha\\)) or when variability is high\n\n\nprimary data\nPrimary data are data you have collected yourself\n\n\nprobability\nRefers to how many times out of a total number of cases a particular event occurs. We can also see it as the chance of a particular event occurring\n\n\nprobability sampling\nIn probability sampling all units have the same probability of entering the sample. In addition, all possible combinations of n cases must have the same probability to be selected\n\n\nQM\nThe process of testing theoretical propositions through the analysis of numerical data in order to provide an answer to a research question\n\n\nquartile\nDivides ordered data into four equal parts and indicates the percentage of observations that falls into the respective quartile and below\n\n\nrange\nThe difference between the largest and the smallest observation\n\n\nredundancy\nTwo or more variables measure the same sub-attribute (Munck & Verkuilen, 2002, p. 13)\n\n\nreliability\nRefers to the extent to which repeated measurement produces the same results\n\n\nrepresentative sample\nA sample which contains all characteristic of the population in accurate proportions\n\n\nresearch question\nA specific enquiry relating to a particular topic or subject. It forms the starting point of the research cycle\n\n\nreverse causality\nCausality runs from x to y. If y could also cause x, then we deal with reverse causality\n\n\nsample\nA sub-group of the population\n\n\nsample distribution\nThe probability distribution of a sample\n\n\nsampling\nThe process of selecting sampling units from the population\n\n\nsampling distribution\nThe probability distribution of a sample statistics, such as the mean. It can be derived from repeated sampling, or by estimation\n\n\nsampling error\nThe extent to which the mean of the population and the mean of the sample differ from one another\n\n\nsampling method\nThe way the sample is created\n\n\nsecondary data\nSecondary data are data which have been collected by somebody else\n\n\nsignificance level\nThe significance level, denoted by \\(\\alpha\\), is the threshold used in hypothesis testing to determine if a result is statistically significant. It represents the probability of rejecting the null hypothesis when it’s actually true (a Type I error). Common levels are 0.05 or 0.01, indicating 5% or 1% risk. We will cover this properly in Week 9.\n\n\nsignificance test\nA significance test is a statistical method used to determine whether observed data provide enough evidence to reject a null hypothesis. It calculates a probability of observing data as extreme as, or more extreme than, the actual sample results, assuming the null hypothesis is true\n\n\nSocial Sciences\nAre concerned with the study of society and seek to scientifically describe and explain the behaviour of actors\n\n\nspurious\nA relationship between two variables, \\(x_1\\) and \\(y\\), is said to be spurious when it appears to exist in a simple analysis but disappears or weakens substantially once a third variable, \\(x_2\\), is introduced into the analysis. This suggests that the observed association between \\(x_1\\) and \\(y\\) was not causal, but rather the result of a confounding influence from \\(x_2\\)\n\n\nstandard deviation\nThe standard deviation s is defined as \\[\\begin{equation*}s=\\sqrt{\\frac{\\text{sum of squared deviations}}{\\text{sample size} -1}}=\\sqrt{\\frac{\\Sigma(y_{i} - \\bar{y})^2}{n-1}}\\end{equation*}\\]\n\n\nstandard error\nThe standard deviation of the sampling distribution. It is defined as:\\[\\begin{equation*}\\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}\\end{equation*}\\]\n\n\nsuppressor\nA suppressor variable is a variable, \\(x_2\\), that increases the predictive validity of another variable, \\(x_1\\), on an outcome \\(y\\) by accounting for irrelevant or misleading variance in \\(x_1\\). In other words, the relationship between \\(x_1\\) and \\(y\\) becomes stronger or more apparent when \\(x_2\\) is included in the analysis, even though \\(x_2\\) may have little or no direct relationship with \\(y\\)\n\n\nsymmetry\nIn the context of causation, symmetry is understood as the law-like regularity of events. There needs to be a recipe (causal mechanism) which regularly produces effects from causes (see Brady (2011))\n\n\nsystematized concept\nA specific formulation of a concept used by a given scholar or group of scholars; commonly involves an explicit definition (Adcock & Collier, 2001, p. 531)\n\n\nt-distribution\nThe t-Distribution is bell-shaped and symmetrical around a mean of zero. Its shape is dependent on the degrees of freedom in the estimation process.\n\n\ntest statistic\nA test statistic is a value calculated from the sample data that is used to decide whether to reject the null hypothesis (H\\(_0\\)) in a hypothesis test. It quantifies the degree to which the observed data diverges from what is expected under the null hypothesis. In a t-test, the test statistic is a t-value, which measures the distance between the sample mean and the (hypothesised) population mean, expressed in units of standard errors.\n\n\ntheory\nA formal set of ideas that is intended to explain why something happens or exists (Oxford Learner’s Dictionaries, n.d.)\n\n\nType I Error\nA Type I Error occurs when a null hypothesis (H\\(_0\\)) that is actually true is incorrectly rejected. It is also known as a false positive errors, as it suggests that an effect of difference exists, when, in fact, it does not. The probability of committing a Type I Error is denoted by the significance level (\\(\\alpha\\)) of the test, which is typically set before conducting the test (e.g. \\(\\alpha\\) = 0.05). This means that there is a 5% chance of rejecting the true null hypothesis.\n\n\nType II Error\nA Type II Error occurs when a null hypothesis (H\\(_0\\)) that is actually false is incorrectly accepted (or not rejected). It is also known as a false negative error, as it suggests that no effect or difference exists when, in fact, there is one.\n\n\nvalidity\nThe extent to which the measure (variable) you choose genuinely represents the concept in question. The word comes from the Latin word “validus” which means “strong”\n\n\nvariable\nAn element of a conceptual component which varies. We also call these “measures”\n\n\nvariance\nIs equal to the squared standard deviation\n\n\nz-score\nThe z-score, sometimes also referred to as z-value, expresses in units of standard deviation how far an observation of interest falls away from the mean. It is defined as \\[\\begin{equation*}z = \\frac{\\text{observation} - \\text{mean}}{\\text{standard deviation}}=\\frac{y-\\mu}{\\sigma}\\end{equation*}\\]\n\n\n\n\n\n\nTable 1: Glossary for PO11Q\n\n\n\n\n\n\n\n\n\nAdcock, R., & Collier, D. (2001). Measurement Validity: A Shared Standard for Qualitative and Quantitative Research. American Political Science Review, 95(3), 529–546. https://doi.org/10.1017/S0003055401003100\n\n\nBrady, H. E. (2011). Causation and Explanation in Social Science. In R. Goodin (Ed.), The Oxford Handbook of Political Science. Oxford University Press. https://doi.org/10.1093/oxfordhb/9780199604456.013.0049\n\n\nClark, T., Foster, L., & Bryman, A. (2021). Bryman’s Social Research Methods (Sixth Edition). Oxford: Oxford University Press.\n\n\nLinke, F. (forthcoming). Introduction to Quantitative Methods in the Social Sciences. Oxford: Oxford University Press.\n\n\nMunck, G. L., & Verkuilen, J. (2002). Conceptualizing and Measuring Democracy: Evaluating Alternative Indices. Comparative Political Studies, 35(5), 5–34. https://doi.org/10.1177/0010414002035001001\n\n\nOxford Learner’s Dictionaries. (n.d.). https://www.oxfordlearnersdictionaries.com/",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Full Glossary</span>"
    ]
  },
  {
    "objectID": "13-bib.html",
    "href": "13-bib.html",
    "title": "List of References",
    "section": "",
    "text": "List of References\n\n\nAdcock, R., & Collier, D. (2001). Measurement\nValidity: A Shared Standard for Qualitative and Quantitative\nResearch. American Political Science\nReview, 95(3), 529–546. https://doi.org/10.1017/S0003055401003100\n\n\nAgresti, A. (2018). Statistical Methods for the\nSocial Sciences (Fifth Edition). Harlow: Pearson.\n\n\nBoix, C., & Stokes, S. (2003). Endogenous\nDemocratization. World Politics,\n55, 517–549. https://doi.org/https://doi.org/10.1353/wp.2003.0019\n\n\nBrady, H. E. (2011). Causation and Explanation in\nSocial Science. In R. Goodin (Ed.), The\nOxford Handbook of Political Science. Oxford University\nPress. https://doi.org/10.1093/oxfordhb/9780199604456.013.0049\n\n\nButton, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint,\nJ., Robinson, E. S. J., & Munafò, M. R. (2013). Power failure: Why\nsmall sample size undermines the reliability of neuroscience. Nature\nReviews Neuroscience, 14(5), 365–376. https://doi.org/10.1038/nrn3475\n\n\nClark, T., Foster, L., & Bryman, A. (2021). Bryman’s Social Research Methods (Sixth\nEdition). Oxford: Oxford University Press.\n\n\nCohen, J. (2013). Statistical power analysis for the behavioral\nsciences. Routledge. https://doi.org/10.4324/9780203771587\n\n\nDorff, C. (2011). Replication data for:\nEthnicity, Insurgency, and Civil War by Fearon and Laitin.\nV2. Harvard Dataverse. https://doi.org/10.2307/420301\n\n\nEpstein, D. L., Bates, R., Goldstone, J., Kristensen, I., &\nO’Halloran, S. (2006). Democratic Transitions.\nAmerican Journal of Political Science,\n50(3), 551–569. https://doi.org/10.1111/j.1540-5907.2006.00201.x\n\n\nEuropean Comission. (n.d.). Eurostat – Your Key\nto European Statistics. https://ec.europa.eu/eurostat/data/database\n\n\nFearon, J. D., & Laitin, D. D. (2003). Ethnicity, Insurgency, and Civil War. American\nPolitical Science Review, 97(1), 75–90. https://doi.org/10.1017/s0003055403000534\n\n\nFogarty, B. J. (2023). Quantitative Social Science Data With\nR (Second Edition). Thousand Oaks, CA: Sage.\n\n\nKing, G. (1995). Replication, replication.\nPS: Political Science and Politics, 28(3), 541–559. https://doi.org/10.2307/420301\n\n\nLakens, D. (2013). Calculating and reporting effect sizes to facilitate\ncumulative science: A practical primer for t-tests and ANOVAs.\nFrontiers in Psychology, 4. https://doi.org/10.3389/fpsyg.2013.00863\n\n\nLinke, F. (forthcoming). Introduction to\nQuantitative Methods in the Social Sciences. Oxford: Oxford\nUniversity Press.\n\n\nMunck, G. L., & Verkuilen, J. (2002). Conceptualizing and Measuring Democracy: Evaluating\nAlternative Indices. Comparative Political Studies,\n35(5), 5–34. https://doi.org/10.1177/0010414002035001001\n\n\nOxford Learner’s Dictionaries. (n.d.). https://www.oxfordlearnersdictionaries.com/\n\n\nPrzeworski, A., Alvarez, M. E., Cheibub, J. A., & Limongi, F.\n(2000). Democracy and Development - Political\nInstitutions and Well-Being in the World, 1950-1990.\nCambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511804946\n\n\nStimson, J. A. (n.d.). Professional Writing in\nPolitical Science: A Highly opinionated Essay. http://stimson.web.unc.edu/files/2018/02/Writing.pdf\n\n\nTufte, E. R. (2001). The Visual Display of\nQuantitative Information (Second Edition). Cheshire, Conn:\nGraphics Press. https://doi.org/10.1198/tech.2002.s78",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>List of References</span>"
    ]
  }
]